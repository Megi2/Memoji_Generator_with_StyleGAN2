{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG7ZEc_982io"
      },
      "source": [
        "# StyleGAN2-ADA-PyTorch\n",
        "\n",
        "**Notes**\n",
        "* Training and Inference sections should be fairly stable. I’ll slowly add new features but it should work for most mainstream use cases.\n",
        "* Advanced Features are being documented toward the bottom of this notebook\n",
        "\n",
        "---\n",
        "\n",
        "If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj4PG4_i9Alt"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGEXPcFJ9UTY"
      },
      "source": [
        "Let’s start by checking to see what GPU we’ve been assigned. Ideally we get a V100, but a P100 is fine too. Other GPUs may lead to issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VVICTCvd4mc",
        "outputId": "36e05f36-f6dd-40a7-895a-78220cd7b2a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: A100-SXM4-40GB (UUID: GPU-41237241-09ea-5e1c-1541-4ba40d5f4636)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSV_HEoD9dxo"
      },
      "source": [
        "Next let’s connect our Google Drive account. This is optional but highly recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuVPuJmbigRs",
        "outputId": "3fdaefe5-ddf7-41e7-cd96-761f38b8e0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTjVmfSK9CYa"
      },
      "source": [
        "## Install repo\n",
        "\n",
        "The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ADVNpBh8Ox",
        "outputId": "c10d8746-4daa-4d81-c68f-f3375bf62ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBeDEGqEbmLy",
        "outputId": "707704fc-2d14-411c-d891-77622e078b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.3.25\n",
            "Uninstalling jax-0.3.25:\n",
            "  Successfully uninstalled jax-0.3.25\n",
            "Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "  Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jax[cuda11_cudnn805]==0.3.10\n",
            "  Downloading jax-0.3.10.tar.gz (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.21.6)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.7.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (4.4.0)\n",
            "Collecting jaxlib==0.3.10+cuda11.cudnn805\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.10%2Bcuda11.cudnn805-cp38-none-manylinux2014_x86_64.whl (175.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.3.10+cuda11.cudnn805->jax[cuda11_cudnn805]==0.3.10) (1.12)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.3.10-py3-none-any.whl size=1088066 sha256=90e99ca7b56e56e0d4e8c18335374c6dd5d1903acf3520dbe88bec1f26a8d852\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/47/10/cbe355213ac14e3fb3cf4ebee40eb28f264cbb45588c78c629\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "Successfully installed jax-0.3.10 jaxlib-0.3.10+cuda11.cudnn805\n",
            "Found existing installation: torch 1.13.1+cu116\n",
            "Uninstalling torch-1.13.1+cu116:\n",
            "  Successfully uninstalled torch-1.13.1+cu116\n",
            "Found existing installation: torchvision 0.14.1+cu116\n",
            "Uninstalling torchvision-0.14.1+cu116:\n",
            "  Successfully uninstalled torchvision-0.14.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp38-cp38-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2041339904 bytes == 0x3ac0000 @  0x7f37f8bca680 0x7f37f8beb824 0x5b3128 0x5bbc90 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2551676928 bytes == 0x7d586000 @  0x7f37f8bca680 0x7f37f8beada2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n",
            "tcmalloc: large alloc 2041339904 bytes == 0x3ac0000 @  0x7f37f8bca680 0x7f37f8beb824 0x5f97c1 0x5f8ecc 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x50b32c 0x5f6b7b 0x66731d 0x5f6706 0x571143 0x50b22e 0x570b82 0x569d8a 0x50b3a0 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x501044 0x56be83 0x501044 0x56be83 0x501044 0x56be83 0x5f5ee6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m863.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp38-cp38-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0+cu111) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.9.0+cu111 torchvision-0.10.0+cu111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja==1.10.2\n",
            "  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading opensimplex-0.4.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.4.12) (0.10.0+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from timm==0.4.12) (1.9.0+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy==6.1.1) (0.2.5)\n",
            "Collecting numpy>=1.22\n",
            "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->timm==0.4.12) (4.4.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.12) (7.1.2)\n",
            "Installing collected packages: ninja, numpy, ftfy, opensimplex, timm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ftfy-6.1.1 ninja-1.10.2 numpy-1.24.1 opensimplex-0.4.4 timm-0.4.12\n"
          ]
        }
      ],
      "source": [
        "#Uninstall new JAX\n",
        "!pip uninstall jax jaxlib -y\n",
        "#GPU frontend\n",
        "!pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "#CPU frontend\n",
        "#!pip install jax[cpu]==0.3.10\n",
        "#Downgrade Pytorch\n",
        "!pip uninstall torch torchvision -y\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install timm==0.4.12 ftfy==6.1.1 ninja==1.10.2 opensimplex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jMmUpn4DWRe"
      },
      "source": [
        "You probably don’t need to run this, but this will update your repo to the latest and greatest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV9bdvzeDRPd",
        "outputId": "a56025b7-a7c3-4ccc-bf59-77cf334d45bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Already up to date.\n",
            "Saved working directory and index state WIP on main: 59e05bb added opensimplex\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "!git stash\n",
        "!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZkcJ58P97Ls"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-h6FpB9FaK"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNc-3wTO-MUd"
      },
      "source": [
        "Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n",
        "\n",
        "* `dataset_path`: this is the path to your .zip file\n",
        "* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`\n",
        "* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV0W6yxP-UIn"
      },
      "outputs": [],
      "source": [
        "#required: definitely edit these!\n",
        "dataset_path = '/content/drive/MyDrive/KHUDA_winter/Memoji.zip'\n",
        "resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results2/00001-Memoji-mirror-paper256-gamma50-bg-resumeffhq256/network-snapshot-000435.pkl'\n",
        "aug_strength = 0.328\n",
        "train_count = 435\n",
        "mirror_x = True\n",
        "#mirror_y = False\n",
        "\n",
        "#optional: you might not need to edit these\n",
        "gamma_value = 50.0\n",
        "augs = 'bg'\n",
        "config = 'paper256'\n",
        "snapshot_count = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-M7WnnfMDI",
        "outputId": "bb8ed8ed-608b-4dd4-ab86-a7ee8f5dbe05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 4,\n",
            "  \"network_snapshot_ticks\": 4,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/KHUDA_winter/Memoji.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 300,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 8\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 50.0\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 64,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 20,\n",
            "  \"ema_rampup\": null,\n",
            "  \"nimg\": 435000,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_p\": 0.328,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results2/00001-Memoji-mirror-paper256-gamma50-bg-resumeffhq256/network-snapshot-000435.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"./results2/00002-Memoji-mirror-paper256-gamma50-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   ./results2/00002-Memoji-mirror-paper256-gamma50-bg-resumecustom\n",
            "Training data:      /content/drive/MyDrive/KHUDA_winter/Memoji.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   300\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  600\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "starting G epochs:  1.74\n",
            "Resuming from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results2/00001-Memoji-mirror-paper256-gamma50-bg-resumeffhq256/network-snapshot-000435.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           262656      -        [8, 512]            float32 \n",
            "mapping.fc1           262656      -        [8, 512]            float32 \n",
            "mapping.fc2           262656      -        [8, 512]            float32 \n",
            "mapping.fc3           262656      -        [8, 512]            float32 \n",
            "mapping.fc4           262656      -        [8, 512]            float32 \n",
            "mapping.fc5           262656      -        [8, 512]            float32 \n",
            "mapping.fc6           262656      -        [8, 512]            float32 \n",
            "mapping.fc7           262656      -        [8, 512]            float32 \n",
            "mapping               -           512      [8, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [8, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [8, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [8, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 24767458    175568   -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   256         16       [8, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [8, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [8, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [8, 128, 128, 128]  float16 \n",
            "b256           -           16       [8, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [8, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [8, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [8, 256, 64, 64]    float16 \n",
            "b128           -           16       [8, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [8, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [8, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [8, 512, 32, 32]    float16 \n",
            "b64            -           16       [8, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n",
            "b32            -           16       [8, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n",
            "b16            -           16       [8, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n",
            "b8             -           16       [8, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [8, 512]            float32 \n",
            "b4.out         513         -        [8, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          24001089    416      -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2023-01-18 11:42:12.389829: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 435.1    time 56s          sec/tick 4.9     sec/kimg 76.84   maintenance 51.2   cpumem 7.82   gpumem 34.65  augment 0.328\n",
            "tick 1     kimg 439.1    time 2m 24s       sec/tick 84.4    sec/kimg 20.92   maintenance 3.5    cpumem 7.85   gpumem 3.03   augment 0.325\n",
            "tick 2     kimg 443.1    time 3m 49s       sec/tick 85.0    sec/kimg 21.08   maintenance 0.0    cpumem 7.85   gpumem 3.01   augment 0.333\n",
            "tick 3     kimg 447.2    time 5m 14s       sec/tick 85.3    sec/kimg 21.15   maintenance 0.0    cpumem 7.85   gpumem 2.99   augment 0.333\n",
            "tick 4     kimg 451.2    time 6m 39s       sec/tick 85.3    sec/kimg 21.14   maintenance 0.0    cpumem 7.85   gpumem 2.99   augment 0.328\n",
            "tick 5     kimg 455.2    time 8m 08s       sec/tick 85.0    sec/kimg 21.09   maintenance 3.6    cpumem 8.20   gpumem 3.00   augment 0.333\n",
            "tick 6     kimg 459.3    time 9m 34s       sec/tick 85.4    sec/kimg 21.19   maintenance 0.0    cpumem 8.20   gpumem 3.01   augment 0.331\n",
            "tick 7     kimg 463.3    time 10m 59s      sec/tick 85.5    sec/kimg 21.20   maintenance 0.0    cpumem 8.20   gpumem 3.00   augment 0.325\n",
            "tick 8     kimg 467.3    time 12m 25s      sec/tick 85.7    sec/kimg 21.27   maintenance 0.0    cpumem 8.20   gpumem 2.99   augment 0.325\n",
            "tick 9     kimg 471.4    time 13m 54s      sec/tick 85.2    sec/kimg 21.13   maintenance 3.6    cpumem 8.21   gpumem 3.00   augment 0.346\n",
            "tick 10    kimg 475.4    time 15m 19s      sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.21   gpumem 3.02   augment 0.346\n",
            "tick 11    kimg 479.4    time 16m 44s      sec/tick 85.5    sec/kimg 21.20   maintenance 0.0    cpumem 8.21   gpumem 3.02   augment 0.341\n",
            "tick 12    kimg 483.4    time 18m 10s      sec/tick 85.8    sec/kimg 21.29   maintenance 0.0    cpumem 8.21   gpumem 3.00   augment 0.336\n",
            "tick 13    kimg 487.5    time 19m 39s      sec/tick 85.7    sec/kimg 21.25   maintenance 3.5    cpumem 8.56   gpumem 3.00   augment 0.341\n",
            "tick 14    kimg 491.5    time 21m 05s      sec/tick 85.6    sec/kimg 21.23   maintenance 0.0    cpumem 8.56   gpumem 3.03   augment 0.333\n",
            "tick 15    kimg 495.5    time 22m 31s      sec/tick 85.7    sec/kimg 21.26   maintenance 0.0    cpumem 8.56   gpumem 3.01   augment 0.343\n",
            "tick 16    kimg 499.6    time 23m 57s      sec/tick 85.7    sec/kimg 21.26   maintenance 0.0    cpumem 8.56   gpumem 3.01   augment 0.343\n",
            "tick 17    kimg 503.6    time 25m 25s      sec/tick 85.3    sec/kimg 21.16   maintenance 3.5    cpumem 8.56   gpumem 3.01   augment 0.343\n",
            "tick 18    kimg 507.6    time 26m 52s      sec/tick 86.5    sec/kimg 21.46   maintenance 0.0    cpumem 8.56   gpumem 3.02   augment 0.341\n",
            "tick 19    kimg 511.7    time 28m 18s      sec/tick 86.4    sec/kimg 21.44   maintenance 0.0    cpumem 8.56   gpumem 2.99   augment 0.343\n",
            "tick 20    kimg 515.7    time 29m 45s      sec/tick 86.6    sec/kimg 21.49   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.333\n",
            "tick 21    kimg 519.7    time 31m 14s      sec/tick 85.9    sec/kimg 21.30   maintenance 3.5    cpumem 8.56   gpumem 3.01   augment 0.333\n",
            "tick 22    kimg 523.8    time 32m 41s      sec/tick 86.6    sec/kimg 21.47   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.325\n",
            "tick 23    kimg 527.8    time 34m 07s      sec/tick 86.5    sec/kimg 21.45   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.325\n",
            "tick 24    kimg 531.8    time 35m 34s      sec/tick 86.4    sec/kimg 21.42   maintenance 0.0    cpumem 8.56   gpumem 2.99   augment 0.331\n",
            "tick 25    kimg 535.9    time 37m 03s      sec/tick 85.9    sec/kimg 21.31   maintenance 3.6    cpumem 8.56   gpumem 3.00   augment 0.336\n",
            "tick 26    kimg 539.9    time 38m 29s      sec/tick 86.0    sec/kimg 21.33   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.338\n",
            "tick 27    kimg 543.9    time 39m 55s      sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.338\n",
            "tick 28    kimg 548.0    time 41m 21s      sec/tick 86.1    sec/kimg 21.36   maintenance 0.0    cpumem 8.56   gpumem 2.99   augment 0.338\n",
            "tick 29    kimg 552.0    time 42m 51s      sec/tick 86.2    sec/kimg 21.38   maintenance 3.5    cpumem 8.56   gpumem 3.01   augment 0.338\n",
            "tick 30    kimg 556.0    time 44m 18s      sec/tick 86.8    sec/kimg 21.54   maintenance 0.0    cpumem 8.56   gpumem 3.03   augment 0.341\n",
            "tick 31    kimg 560.1    time 45m 45s      sec/tick 86.8    sec/kimg 21.53   maintenance 0.0    cpumem 8.56   gpumem 3.00   augment 0.351\n",
            "tick 32    kimg 564.1    time 47m 11s      sec/tick 86.7    sec/kimg 21.51   maintenance 0.0    cpumem 8.56   gpumem 3.02   augment 0.341\n",
            "tick 33    kimg 568.1    time 48m 40s      sec/tick 85.6    sec/kimg 21.23   maintenance 3.5    cpumem 8.57   gpumem 3.01   augment 0.336\n",
            "tick 34    kimg 572.2    time 50m 07s      sec/tick 86.4    sec/kimg 21.43   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.333\n",
            "tick 35    kimg 576.2    time 51m 34s      sec/tick 86.7    sec/kimg 21.50   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.318\n",
            "tick 36    kimg 580.2    time 53m 00s      sec/tick 86.3    sec/kimg 21.40   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.338\n",
            "tick 37    kimg 584.2    time 54m 30s      sec/tick 86.3    sec/kimg 21.41   maintenance 3.5    cpumem 8.57   gpumem 3.01   augment 0.338\n",
            "tick 38    kimg 588.3    time 55m 56s      sec/tick 86.5    sec/kimg 21.46   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.356\n",
            "tick 39    kimg 592.3    time 57m 23s      sec/tick 86.4    sec/kimg 21.44   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.348\n",
            "tick 40    kimg 596.3    time 58m 49s      sec/tick 86.0    sec/kimg 21.32   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.359\n",
            "tick 41    kimg 600.4    time 1h 00m 18s   sec/tick 85.6    sec/kimg 21.23   maintenance 3.5    cpumem 8.57   gpumem 3.00   augment 0.354\n",
            "tick 42    kimg 604.4    time 1h 01m 43s   sec/tick 85.7    sec/kimg 21.25   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.351\n",
            "tick 43    kimg 608.4    time 1h 03m 09s   sec/tick 85.7    sec/kimg 21.25   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.341\n",
            "tick 44    kimg 612.5    time 1h 04m 35s   sec/tick 85.7    sec/kimg 21.26   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.341\n",
            "tick 45    kimg 616.5    time 1h 06m 04s   sec/tick 85.5    sec/kimg 21.20   maintenance 3.5    cpumem 8.57   gpumem 3.00   augment 0.351\n",
            "tick 46    kimg 620.5    time 1h 07m 30s   sec/tick 86.2    sec/kimg 21.37   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.348\n",
            "tick 47    kimg 624.6    time 1h 08m 56s   sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.356\n",
            "tick 48    kimg 628.6    time 1h 10m 22s   sec/tick 86.2    sec/kimg 21.37   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.356\n",
            "tick 49    kimg 632.6    time 1h 11m 51s   sec/tick 85.2    sec/kimg 21.14   maintenance 3.5    cpumem 8.57   gpumem 3.01   augment 0.351\n",
            "tick 50    kimg 636.7    time 1h 13m 18s   sec/tick 86.8    sec/kimg 21.54   maintenance 0.0    cpumem 8.57   gpumem 3.02   augment 0.341\n",
            "tick 51    kimg 640.7    time 1h 14m 44s   sec/tick 86.0    sec/kimg 21.34   maintenance 0.0    cpumem 8.57   gpumem 3.02   augment 0.336\n",
            "tick 52    kimg 644.7    time 1h 16m 09s   sec/tick 85.9    sec/kimg 21.30   maintenance 0.0    cpumem 8.57   gpumem 3.02   augment 0.331\n",
            "tick 53    kimg 648.8    time 1h 17m 39s   sec/tick 85.8    sec/kimg 21.28   maintenance 3.5    cpumem 8.57   gpumem 3.02   augment 0.331\n",
            "tick 54    kimg 652.8    time 1h 19m 05s   sec/tick 86.1    sec/kimg 21.35   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.323\n",
            "tick 55    kimg 656.8    time 1h 20m 31s   sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.57   gpumem 3.00   augment 0.338\n",
            "tick 56    kimg 660.9    time 1h 21m 57s   sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.57   gpumem 3.03   augment 0.333\n",
            "tick 57    kimg 664.9    time 1h 23m 26s   sec/tick 85.5    sec/kimg 21.20   maintenance 3.5    cpumem 8.57   gpumem 3.02   augment 0.338\n",
            "tick 58    kimg 668.9    time 1h 24m 52s   sec/tick 86.1    sec/kimg 21.35   maintenance 0.0    cpumem 8.57   gpumem 3.01   augment 0.346\n",
            "tick 59    kimg 673.0    time 1h 26m 18s   sec/tick 86.2    sec/kimg 21.37   maintenance 0.0    cpumem 8.57   gpumem 3.02   augment 0.361\n",
            "tick 60    kimg 677.0    time 1h 27m 44s   sec/tick 85.8    sec/kimg 21.28   maintenance 0.0    cpumem 8.57   gpumem 3.03   augment 0.366\n",
            "tick 61    kimg 681.0    time 1h 29m 12s   sec/tick 85.2    sec/kimg 21.13   maintenance 3.5    cpumem 8.58   gpumem 3.02   augment 0.361\n",
            "tick 62    kimg 685.0    time 1h 30m 38s   sec/tick 85.1    sec/kimg 21.11   maintenance 0.0    cpumem 8.58   gpumem 3.02   augment 0.369\n",
            "tick 63    kimg 689.1    time 1h 32m 03s   sec/tick 85.4    sec/kimg 21.17   maintenance 0.0    cpumem 8.58   gpumem 3.02   augment 0.374\n",
            "tick 64    kimg 693.1    time 1h 33m 28s   sec/tick 85.4    sec/kimg 21.17   maintenance 0.0    cpumem 8.58   gpumem 3.02   augment 0.374\n",
            "tick 65    kimg 697.1    time 1h 34m 56s   sec/tick 84.4    sec/kimg 20.93   maintenance 3.5    cpumem 8.59   gpumem 3.01   augment 0.384\n",
            "tick 66    kimg 701.2    time 1h 36m 21s   sec/tick 85.3    sec/kimg 21.16   maintenance 0.0    cpumem 8.59   gpumem 3.01   augment 0.372\n",
            "tick 67    kimg 705.2    time 1h 37m 47s   sec/tick 85.6    sec/kimg 21.22   maintenance 0.0    cpumem 8.59   gpumem 3.02   augment 0.372\n",
            "tick 68    kimg 709.2    time 1h 39m 13s   sec/tick 86.2    sec/kimg 21.39   maintenance 0.0    cpumem 8.59   gpumem 3.03   augment 0.372\n",
            "tick 69    kimg 713.3    time 1h 40m 42s   sec/tick 85.4    sec/kimg 21.18   maintenance 3.5    cpumem 8.59   gpumem 3.01   augment 0.366\n",
            "tick 70    kimg 717.3    time 1h 42m 07s   sec/tick 85.0    sec/kimg 21.08   maintenance 0.0    cpumem 8.59   gpumem 3.02   augment 0.359\n",
            "tick 71    kimg 721.3    time 1h 43m 32s   sec/tick 85.0    sec/kimg 21.09   maintenance 0.0    cpumem 8.59   gpumem 3.00   augment 0.348\n",
            "tick 72    kimg 725.4    time 1h 44m 58s   sec/tick 85.3    sec/kimg 21.16   maintenance 0.0    cpumem 8.59   gpumem 3.01   augment 0.356\n",
            "tick 73    kimg 729.4    time 1h 46m 26s   sec/tick 85.1    sec/kimg 21.11   maintenance 3.4    cpumem 8.60   gpumem 3.03   augment 0.356\n",
            "tick 74    kimg 733.4    time 1h 47m 51s   sec/tick 85.2    sec/kimg 21.14   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.348\n",
            "tick 75    kimg 737.5    time 1h 49m 17s   sec/tick 86.1    sec/kimg 21.35   maintenance 0.0    cpumem 8.60   gpumem 3.04   augment 0.364\n",
            "tick 76    kimg 741.5    time 1h 50m 43s   sec/tick 85.1    sec/kimg 21.10   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.379\n",
            "tick 77    kimg 745.5    time 1h 52m 11s   sec/tick 85.3    sec/kimg 21.14   maintenance 3.5    cpumem 8.60   gpumem 3.01   augment 0.369\n",
            "tick 78    kimg 749.6    time 1h 53m 37s   sec/tick 85.6    sec/kimg 21.23   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.382\n",
            "tick 79    kimg 753.6    time 1h 55m 03s   sec/tick 86.4    sec/kimg 21.42   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.382\n",
            "tick 80    kimg 757.6    time 1h 56m 30s   sec/tick 86.5    sec/kimg 21.46   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.377\n",
            "tick 81    kimg 761.7    time 1h 57m 58s   sec/tick 85.0    sec/kimg 21.08   maintenance 3.5    cpumem 8.60   gpumem 3.02   augment 0.382\n",
            "tick 82    kimg 765.7    time 1h 59m 24s   sec/tick 85.8    sec/kimg 21.27   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.374\n",
            "tick 83    kimg 769.7    time 2h 00m 49s   sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.379\n",
            "tick 84    kimg 773.8    time 2h 02m 15s   sec/tick 85.6    sec/kimg 21.22   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.374\n",
            "tick 85    kimg 777.8    time 2h 03m 44s   sec/tick 85.3    sec/kimg 21.15   maintenance 3.5    cpumem 8.60   gpumem 3.01   augment 0.369\n",
            "tick 86    kimg 781.8    time 2h 05m 09s   sec/tick 85.3    sec/kimg 21.16   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.372\n",
            "tick 87    kimg 785.8    time 2h 06m 35s   sec/tick 85.8    sec/kimg 21.28   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.377\n",
            "tick 88    kimg 789.9    time 2h 08m 01s   sec/tick 86.0    sec/kimg 21.32   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.377\n",
            "tick 89    kimg 793.9    time 2h 09m 29s   sec/tick 85.0    sec/kimg 21.08   maintenance 3.5    cpumem 8.60   gpumem 3.00   augment 0.382\n",
            "tick 90    kimg 797.9    time 2h 10m 55s   sec/tick 85.2    sec/kimg 21.14   maintenance 0.0    cpumem 8.60   gpumem 3.07   augment 0.379\n",
            "tick 91    kimg 802.0    time 2h 12m 20s   sec/tick 85.1    sec/kimg 21.11   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.374\n",
            "tick 92    kimg 806.0    time 2h 13m 45s   sec/tick 85.7    sec/kimg 21.25   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.379\n",
            "tick 93    kimg 810.0    time 2h 15m 14s   sec/tick 84.9    sec/kimg 21.05   maintenance 3.5    cpumem 8.60   gpumem 3.03   augment 0.374\n",
            "tick 94    kimg 814.1    time 2h 16m 40s   sec/tick 86.2    sec/kimg 21.38   maintenance 0.0    cpumem 8.60   gpumem 3.05   augment 0.372\n",
            "tick 95    kimg 818.1    time 2h 18m 06s   sec/tick 86.2    sec/kimg 21.37   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.372\n",
            "tick 96    kimg 822.1    time 2h 19m 33s   sec/tick 86.6    sec/kimg 21.47   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.372\n",
            "tick 97    kimg 826.2    time 2h 21m 02s   sec/tick 85.9    sec/kimg 21.31   maintenance 3.5    cpumem 8.60   gpumem 3.01   augment 0.372\n",
            "tick 98    kimg 830.2    time 2h 22m 28s   sec/tick 86.3    sec/kimg 21.42   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.369\n",
            "tick 99    kimg 834.2    time 2h 23m 55s   sec/tick 86.9    sec/kimg 21.55   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.379\n",
            "tick 100   kimg 838.3    time 2h 25m 22s   sec/tick 86.5    sec/kimg 21.45   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.384\n",
            "tick 101   kimg 842.3    time 2h 26m 51s   sec/tick 85.7    sec/kimg 21.26   maintenance 3.5    cpumem 8.60   gpumem 3.03   augment 0.389\n",
            "tick 102   kimg 846.3    time 2h 28m 17s   sec/tick 86.0    sec/kimg 21.33   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.377\n",
            "tick 103   kimg 850.4    time 2h 29m 43s   sec/tick 86.0    sec/kimg 21.33   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.366\n",
            "tick 104   kimg 854.4    time 2h 31m 09s   sec/tick 85.8    sec/kimg 21.28   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.372\n",
            "tick 105   kimg 858.4    time 2h 32m 37s   sec/tick 85.2    sec/kimg 21.14   maintenance 3.5    cpumem 8.60   gpumem 3.02   augment 0.361\n",
            "tick 106   kimg 862.5    time 2h 34m 03s   sec/tick 86.0    sec/kimg 21.33   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.364\n",
            "tick 107   kimg 866.5    time 2h 35m 29s   sec/tick 85.9    sec/kimg 21.30   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.356\n",
            "tick 108   kimg 870.5    time 2h 36m 55s   sec/tick 86.1    sec/kimg 21.35   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.351\n",
            "tick 109   kimg 874.6    time 2h 38m 24s   sec/tick 85.2    sec/kimg 21.12   maintenance 3.5    cpumem 8.60   gpumem 3.00   augment 0.356\n",
            "tick 110   kimg 878.6    time 2h 39m 50s   sec/tick 85.6    sec/kimg 21.23   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.374\n",
            "tick 111   kimg 882.6    time 2h 41m 15s   sec/tick 85.6    sec/kimg 21.23   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.374\n",
            "tick 112   kimg 886.6    time 2h 42m 41s   sec/tick 85.5    sec/kimg 21.20   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.374\n",
            "tick 113   kimg 890.7    time 2h 44m 09s   sec/tick 85.1    sec/kimg 21.12   maintenance 3.5    cpumem 8.60   gpumem 3.02   augment 0.369\n",
            "tick 114   kimg 894.7    time 2h 45m 35s   sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.377\n",
            "tick 115   kimg 898.7    time 2h 47m 01s   sec/tick 86.1    sec/kimg 21.36   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.387\n",
            "tick 116   kimg 902.8    time 2h 48m 27s   sec/tick 85.5    sec/kimg 21.20   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.392\n",
            "tick 117   kimg 906.8    time 2h 49m 55s   sec/tick 84.9    sec/kimg 21.05   maintenance 3.5    cpumem 8.60   gpumem 3.02   augment 0.402\n",
            "tick 118   kimg 910.8    time 2h 51m 21s   sec/tick 85.6    sec/kimg 21.24   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.392\n",
            "tick 119   kimg 914.9    time 2h 52m 46s   sec/tick 85.5    sec/kimg 21.22   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.377\n",
            "tick 120   kimg 918.9    time 2h 54m 12s   sec/tick 85.7    sec/kimg 21.25   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.377\n",
            "tick 121   kimg 922.9    time 2h 55m 41s   sec/tick 85.5    sec/kimg 21.21   maintenance 3.5    cpumem 8.60   gpumem 3.04   augment 0.387\n",
            "tick 122   kimg 927.0    time 2h 57m 07s   sec/tick 85.5    sec/kimg 21.21   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.389\n",
            "tick 123   kimg 931.0    time 2h 58m 33s   sec/tick 85.9    sec/kimg 21.32   maintenance 0.0    cpumem 8.60   gpumem 3.00   augment 0.384\n",
            "tick 124   kimg 935.0    time 2h 59m 58s   sec/tick 85.7    sec/kimg 21.25   maintenance 0.0    cpumem 8.60   gpumem 3.02   augment 0.389\n",
            "tick 125   kimg 939.1    time 3h 01m 28s   sec/tick 86.2    sec/kimg 21.37   maintenance 3.5    cpumem 8.60   gpumem 3.03   augment 0.389\n",
            "tick 126   kimg 943.1    time 3h 02m 54s   sec/tick 86.1    sec/kimg 21.36   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.392\n",
            "tick 127   kimg 947.1    time 3h 04m 19s   sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.60   gpumem 3.01   augment 0.389\n",
            "tick 128   kimg 951.2    time 3h 05m 45s   sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.60   gpumem 3.03   augment 0.400\n",
            "tick 129   kimg 955.2    time 3h 07m 13s   sec/tick 84.5    sec/kimg 20.95   maintenance 3.5    cpumem 8.61   gpumem 3.02   augment 0.400\n",
            "tick 130   kimg 959.2    time 3h 08m 38s   sec/tick 85.6    sec/kimg 21.23   maintenance 0.0    cpumem 8.61   gpumem 3.01   augment 0.397\n",
            "tick 131   kimg 963.3    time 3h 10m 04s   sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.61   gpumem 3.02   augment 0.392\n",
            "tick 132   kimg 967.3    time 3h 11m 29s   sec/tick 85.4    sec/kimg 21.18   maintenance 0.0    cpumem 8.61   gpumem 3.03   augment 0.387\n",
            "tick 133   kimg 971.3    time 3h 12m 58s   sec/tick 85.3    sec/kimg 21.15   maintenance 3.5    cpumem 8.61   gpumem 3.01   augment 0.397\n",
            "tick 134   kimg 975.4    time 3h 14m 23s   sec/tick 85.4    sec/kimg 21.17   maintenance 0.0    cpumem 8.61   gpumem 3.03   augment 0.395\n",
            "tick 135   kimg 979.4    time 3h 15m 49s   sec/tick 85.9    sec/kimg 21.29   maintenance 0.0    cpumem 8.61   gpumem 3.02   augment 0.389\n",
            "tick 136   kimg 983.4    time 3h 17m 15s   sec/tick 85.5    sec/kimg 21.19   maintenance 0.0    cpumem 8.61   gpumem 3.01   augment 0.384\n",
            "tick 137   kimg 987.4    time 3h 18m 44s   sec/tick 85.6    sec/kimg 21.23   maintenance 3.5    cpumem 8.61   gpumem 3.02   augment 0.377\n",
            "tick 138   kimg 991.5    time 3h 20m 10s   sec/tick 85.8    sec/kimg 21.27   maintenance 0.0    cpumem 8.61   gpumem 3.02   augment 0.389\n",
            "tick 139   kimg 995.5    time 3h 21m 35s   sec/tick 85.9    sec/kimg 21.31   maintenance 0.0    cpumem 8.61   gpumem 3.03   augment 0.379\n",
            "tick 140   kimg 999.5    time 3h 23m 01s   sec/tick 86.0    sec/kimg 21.32   maintenance 0.0    cpumem 8.61   gpumem 3.02   augment 0.384\n",
            "tick 141   kimg 1003.6   time 3h 24m 30s   sec/tick 85.1    sec/kimg 21.12   maintenance 3.5    cpumem 8.61   gpumem 3.01   augment 0.374\n",
            "tick 142   kimg 1007.6   time 3h 25m 56s   sec/tick 85.9    sec/kimg 21.29   maintenance 0.0    cpumem 8.61   gpumem 3.01   augment 0.387\n",
            "tick 143   kimg 1011.6   time 3h 27m 22s   sec/tick 85.8    sec/kimg 21.28   maintenance 0.0    cpumem 8.61   gpumem 3.01   augment 0.392\n",
            "tick 144   kimg 1015.7   time 3h 28m 48s   sec/tick 86.0    sec/kimg 21.33   maintenance 0.0    cpumem 8.61   gpumem 3.02   augment 0.397\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ],
      "source": [
        "!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=./results2 --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgvSvfyi_R_-"
      },
      "source": [
        "### Resume Training\n",
        "\n",
        "Once Colab has shutdown, you’ll need to resume your training. Reset the variables above, particularly the `resume_from` and `aug_strength` settings.\n",
        "\n",
        "1. Point `resume_from` to the last .pkl you trained (you’ll find these in the `results` folder)\n",
        "2. Update `aug_strength` to match the augment value of the last pkl file. Often you’ll see this in the console, but you may need to look at the `log.txt`. Updating this makes sure training stays as stable as possible.\n",
        "3. You may want to update `train_count` to keep track of your training progress.\n",
        "\n",
        "Once all of this has been reset, run that variable cell and the training command cell after it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VznRirOE5ENI"
      },
      "source": [
        "## Convert Legacy Model\n",
        "\n",
        "If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n",
        "\n",
        "`--source`: path to model that you want to convert\n",
        "\n",
        "`--dest`: path and file name to convert to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzkP-Rww5Np9"
      },
      "outputs": [],
      "source": [
        "!python legacy.py --source=/content/drive/MyDrive/runway.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/runway.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6EtrPqL9ILk"
      },
      "source": [
        "## Testing/Inference\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdyfH0O8In_"
      },
      "source": [
        "### Generate Single Images\n",
        "\n",
        "`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "`--truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD8eQFOTLD4g",
        "outputId": "0315fab1-be24-479a-a1dd-abbc99748108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYRXenMoZSHf",
        "outputId": "9b4f5293-23a3-4baa-d339-84287067232d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Loading networks from \"/content/drive/MyDrive/KHUDA_winter/ffhq256-model/out.pkl\"...\n",
            "Generating image for seed 9 (0/1) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --outdir=/content/drive/MyDrive/KHUDA_winter/out0121/ --trunc=0.8 --seeds=9 --network=/content/drive/MyDrive/KHUDA_winter/ffhq256-model/out.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EKihY26OmTD"
      },
      "source": [
        "#### Non-Square outputs\n",
        "\n",
        "We can modify the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n",
        "\n",
        "* `--size` size takes in a value of `xdim-ydim`. For example, to generate a 1920x1080 image use `1920-1080`\n",
        "* `--scale-type` This determines the padding style to apply in the additional space. There are four options: `pad`, `padside`, `symm`, and `symmside`. I recommend trying each one to see what works best with your images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qaE0GG3HwaK",
        "outputId": "58083633-d254-4bcf-b949-c3c8cf7113d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.4.4-py3-none-any.whl (19 kB)\n",
            "Collecting numpy>=1.22\n",
            "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, opensimplex\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.1 opensimplex-0.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install opensimplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuJxj-gdO3G0",
        "outputId": "4c10dda2-13db-43a3-8f33-7fe8a45a2400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Traceback (most recent call last):\n",
            "  File \"generate.py\", line 25, in <module>\n",
            "    from opensimplex import OpenSimplex\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/opensimplex/__init__.py\", line 4, in <module>\n",
            "    from .api import *\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/opensimplex/api.py\", line 2, in <module>\n",
            "    from .internals import _init, _noise2, _noise3, _noise4, _noise2a, _noise3a, _noise4a\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/opensimplex/internals.py\", line 9, in <module>\n",
            "    from numba import njit, prange\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/numba/__init__.py\", line 42, in <module>\n",
            "    from numba.np.ufunc import (vectorize, guvectorize, threading_layer,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/numba/np/ufunc/__init__.py\", line 3, in <module>\n",
            "    from numba.np.ufunc.decorators import Vectorize, GUVectorize, vectorize, guvectorize\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/numba/np/ufunc/decorators.py\", line 3, in <module>\n",
            "    from numba.np.ufunc import _internal\n",
            "SystemError: initialization of _internal failed without raising an exception\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --outdir=/content/out/images/ --trunc=0.7 --size=1820-1024 --scale-type=symm --seeds=0-499 --network=/content/crystal.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_sGJowuPwwE"
      },
      "source": [
        "We can use these options for any image or video generation commands (excluding projection)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOTCWVonoVL"
      },
      "source": [
        "### Truncation Traversal\n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "#### Options\n",
        "`--network`: Again, this should be the path to your .pkl file.\n",
        "\n",
        "`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyzdGr7OnrMG"
      },
      "outputs": [],
      "source": [
        "!python generate.py --process=\"truncation\" --outdir=/content/out/trunc-trav-3/ --start=-0.8 --stop=2.8 --increment=0.02 --seeds=470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzj0igO8Lfu"
      },
      "source": [
        "### Interpolations\n",
        "\n",
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at different examples of interpolation below.\n",
        "\n",
        "#### Options\n",
        "\n",
        "`--network`: path to your .pkl file\n",
        "\n",
        "`--interpolation`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
        "\n",
        "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`--trunc`: truncation value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSqafIzNwhx"
      },
      "source": [
        "#### Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqkiskly8S5_",
        "outputId": "9c2bbeda-0d9e-4eb3-f59a-f6cfe57635a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Loading networks from \"/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl\"...\n",
            "Traceback (most recent call last):\n",
            "  File \"generate.py\", line 492, in <module>\n",
            "    generate_images() # pylint: disable=no-value-for-parameter\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/decorators.py\", line 21, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"generate.py\", line 406, in generate_images\n",
            "    with dnnlib.util.open_url(network_pkl) as f:\n",
            "  File \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/dnnlib/util.py\", line 389, in open_url\n",
            "    return url if return_filename else open(url, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl'\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --outdir=/content/out/video1-w-0.5/ --space=\"z\" --trunc=0.5 --process=\"interpolation\" --seeds=463,470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCUEV3aO8s_X"
      },
      "outputs": [],
      "source": [
        "!python generate.py --outdir=out/video1-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi3d7xzpN2Uj"
      },
      "source": [
        "#### Slerp Interpolation\n",
        "\n",
        "This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations ad discussions.\n",
        "\n",
        "In reality I do not find a huge difference between linear and spherical interpolations (the difference in z- and w-space is enough in many cases), but I’ve implemented slerp here for anyone interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0-cUd3fB_kJ",
        "outputId": "ae2b9e01-21d1-4d97-ae83-564d7723557b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Loading networks from \"/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl\"...\n",
            "Traceback (most recent call last):\n",
            "  File \"generate.py\", line 492, in <module>\n",
            "    generate_images() # pylint: disable=no-value-for-parameter\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/decorators.py\", line 21, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"generate.py\", line 406, in generate_images\n",
            "    with dnnlib.util.open_url(network_pkl) as f:\n",
            "  File \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/dnnlib/util.py\", line 389, in open_url\n",
            "    return url if return_filename else open(url, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl'\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --outdir=out/slerp-z/ --space=\"z\" --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --frames=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtnBIF75pcoY",
        "outputId": "7bd7be12-6f14-49c9-b62c-c99563bd2785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Loading networks from \"/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl\"...\n",
            "Traceback (most recent call last):\n",
            "  File \"generate.py\", line 492, in <module>\n",
            "    generate_images() # pylint: disable=no-value-for-parameter\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/decorators.py\", line 21, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"generate.py\", line 406, in generate_images\n",
            "    with dnnlib.util.open_url(network_pkl) as f:\n",
            "  File \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/dnnlib/util.py\", line 389, in open_url\n",
            "    return url if return_filename else open(url, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl'\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --outdir=out/slerp-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --frames=12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1HsU_CPcF5"
      },
      "source": [
        "#### Noise Loop\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "`--interpolation=\"noiseloop\"`: set this to use the noise loop funtion\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n",
        "\n",
        "Noise loops currently only work in z space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfR6DhfvN8b_"
      },
      "outputs": [],
      "source": [
        "!python generate.py --outdir=out/video-noiseloop-0.9d/ --trunc=0.8 --process=\"interpolation\" --interpolation=\"noiseloop\" --diameter=0.9 --random_seed=100 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKFb-4CedOq"
      },
      "source": [
        "#### Circular Loop\n",
        "\n",
        "The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n",
        "\n",
        "I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao62za9_QfOF"
      },
      "outputs": [],
      "source": [
        "!python generate.py --outdir=out/video-circularloop/ --trunc=1 --process=\"interpolation\" --interpolation=\"circularloop\" --diameter=800.00 --frames=720 --random_seed=90 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz-fVtzyAHg1"
      },
      "source": [
        "## Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7tXSpCA_zh"
      },
      "source": [
        "### Basic Projector\n",
        "\n",
        "*   `--target`: this is a path to the image file that you want to \"find\" in your model. This image must be the exact same size as your model.\n",
        "*   `--num-steps`: how many iterations the projctor should run for. Lower will mean less steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p84CtZUGAKnR",
        "outputId": "aed9a701-0519-4e1e-e53e-d124f9742d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: projector.py [OPTIONS]\n",
            "\n",
            "  Project given image to the latent space of\n",
            "  pretrained network pickle.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  python projector.py --outdir=out --target=~/mytargetimg.png \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
            "\n",
            "Options:\n",
            "  --network TEXT        Network pickle filename\n",
            "                        [required]\n",
            "\n",
            "  --target FILE         Target image file to\n",
            "                        project to  [required]\n",
            "\n",
            "  --num-steps INTEGER   Number of optimization\n",
            "                        steps  [default: 1000]\n",
            "\n",
            "  --seed INTEGER        Random seed  [default:\n",
            "                        303]\n",
            "\n",
            "  --save-video BOOLEAN  Save an mp4 video of\n",
            "                        optimization progress\n",
            "                        [default: True]\n",
            "\n",
            "  --outdir DIR          Where to save the output\n",
            "                        images  [required]\n",
            "\n",
            "  --help                Show this message and\n",
            "                        exit.\n"
          ]
        }
      ],
      "source": [
        "!python projector.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ydYh8tcfBL",
        "outputId": "b07d8a5b-2a79-4edf-a0d6-505938b64828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.8\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN6PiAY5cjs3",
        "outputId": "073becfa-853e-4dc5-c2e9-b595a2a0850e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/KHUDA_winter/ffhq-res512-mirror-stylegan2-noaug.pkl\"...\n",
            "Computing W midpoint and stddev using 10000 samples...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "step    1/200: dist 0.68 loss 9557.90\n",
            "step    2/200: dist 0.67 loss 13249.61\n",
            "step    3/200: dist 0.64 loss 11521.96\n",
            "step    4/200: dist 0.72 loss 8738.15\n",
            "step    5/200: dist 0.63 loss 5736.32\n",
            "step    6/200: dist 0.63 loss 3210.39\n",
            "step    7/200: dist 0.61 loss 2003.36\n",
            "step    8/200: dist 0.64 loss 2411.85\n",
            "step    9/200: dist 0.59 loss 4028.92\n",
            "step   10/200: dist 0.58 loss 5838.40\n",
            "step   11/200: dist 0.54 loss 6856.43\n",
            "step   12/200: dist 0.55 loss 6584.67\n",
            "step   13/200: dist 0.56 loss 5787.71\n",
            "step   14/200: dist 0.52 loss 4627.21\n",
            "step   15/200: dist 0.53 loss 3462.55\n",
            "step   16/200: dist 0.51 loss 2563.73\n",
            "step   17/200: dist 0.49 loss 2089.80\n",
            "step   18/200: dist 0.52 loss 1789.88\n",
            "step   19/200: dist 0.50 loss 1552.90\n",
            "step   20/200: dist 0.50 loss 1498.78\n",
            "step   21/200: dist 0.49 loss 1582.69\n",
            "step   22/200: dist 0.50 loss 1601.54\n",
            "step   23/200: dist 0.48 loss 1509.60\n",
            "step   24/200: dist 0.47 loss 1399.36\n",
            "step   25/200: dist 0.47 loss 1370.81\n",
            "step   26/200: dist 0.46 loss 1324.33\n",
            "step   27/200: dist 0.46 loss 1197.22\n",
            "step   28/200: dist 0.46 loss 996.44\n",
            "step   29/200: dist 0.43 loss 779.32\n",
            "step   30/200: dist 0.48 loss 588.03\n",
            "step   31/200: dist 0.46 loss 443.05\n",
            "step   32/200: dist 0.44 loss 353.00\n",
            "step   33/200: dist 0.42 loss 330.54\n",
            "step   34/200: dist 0.43 loss 386.43\n",
            "step   35/200: dist 0.45 loss 447.62\n",
            "step   36/200: dist 0.42 loss 489.31\n",
            "step   37/200: dist 0.42 loss 496.34\n",
            "step   38/200: dist 0.43 loss 436.14\n",
            "step   39/200: dist 0.42 loss 343.31\n",
            "step   40/200: dist 0.42 loss 255.43\n",
            "step   41/200: dist 0.42 loss 178.88\n",
            "step   42/200: dist 0.42 loss 125.29\n",
            "step   43/200: dist 0.41 loss 110.70\n",
            "step   44/200: dist 0.40 loss 138.80\n",
            "step   45/200: dist 0.40 loss 172.50\n",
            "step   46/200: dist 0.40 loss 183.58\n",
            "step   47/200: dist 0.39 loss 166.91\n",
            "step   48/200: dist 0.38 loss 140.40\n",
            "step   49/200: dist 0.38 loss 109.30\n",
            "step   50/200: dist 0.38 loss 83.45\n",
            "step   51/200: dist 0.39 loss 73.11\n",
            "step   52/200: dist 0.37 loss 78.77\n",
            "step   53/200: dist 0.38 loss 79.03\n",
            "step   54/200: dist 0.39 loss 67.79\n",
            "step   55/200: dist 0.39 loss 58.16\n",
            "step   56/200: dist 0.36 loss 54.10\n",
            "step   57/200: dist 0.37 loss 49.26\n",
            "step   58/200: dist 0.36 loss 42.25\n",
            "step   59/200: dist 0.37 loss 38.81\n",
            "step   60/200: dist 0.36 loss 34.97\n",
            "step   61/200: dist 0.36 loss 30.70\n",
            "step   62/200: dist 0.36 loss 27.77\n",
            "step   63/200: dist 0.35 loss 29.92\n",
            "step   64/200: dist 0.35 loss 33.09\n",
            "step   65/200: dist 0.34 loss 29.90\n",
            "step   66/200: dist 0.34 loss 22.13\n",
            "step   67/200: dist 0.35 loss 14.96\n",
            "step   68/200: dist 0.35 loss 9.96 \n",
            "step   69/200: dist 0.35 loss 8.63 \n",
            "step   70/200: dist 0.34 loss 10.89\n",
            "step   71/200: dist 0.34 loss 14.44\n",
            "step   72/200: dist 0.33 loss 15.51\n",
            "step   73/200: dist 0.33 loss 14.19\n",
            "step   74/200: dist 0.33 loss 12.60\n",
            "step   75/200: dist 0.34 loss 13.07\n",
            "step   76/200: dist 0.34 loss 14.32\n",
            "step   77/200: dist 0.35 loss 14.46\n",
            "step   78/200: dist 0.34 loss 14.11\n",
            "step   79/200: dist 0.33 loss 13.84\n",
            "step   80/200: dist 0.33 loss 13.30\n",
            "step   81/200: dist 0.32 loss 14.90\n",
            "step   82/200: dist 0.33 loss 15.71\n",
            "step   83/200: dist 0.34 loss 10.16\n",
            "step   84/200: dist 0.33 loss 6.18 \n",
            "step   85/200: dist 0.33 loss 9.79 \n",
            "step   86/200: dist 0.33 loss 11.77\n",
            "step   87/200: dist 0.33 loss 7.40 \n",
            "step   88/200: dist 0.32 loss 6.08 \n",
            "step   89/200: dist 0.32 loss 7.19 \n",
            "step   90/200: dist 0.32 loss 4.80 \n",
            "step   91/200: dist 0.32 loss 3.64 \n",
            "step   92/200: dist 0.32 loss 4.87 \n",
            "step   93/200: dist 0.32 loss 3.16 \n",
            "step   94/200: dist 0.32 loss 2.46 \n",
            "step   95/200: dist 0.31 loss 5.18 \n",
            "step   96/200: dist 0.32 loss 4.56 \n",
            "step   97/200: dist 0.31 loss 1.80 \n",
            "step   98/200: dist 0.32 loss 2.72 \n",
            "step   99/200: dist 0.31 loss 3.72 \n",
            "step  100/200: dist 0.31 loss 2.12 \n",
            "step  101/200: dist 0.31 loss 1.84 \n",
            "step  102/200: dist 0.31 loss 2.84 \n",
            "step  103/200: dist 0.31 loss 2.92 \n",
            "step  104/200: dist 0.31 loss 3.64 \n",
            "step  105/200: dist 0.31 loss 4.91 \n",
            "step  106/200: dist 0.31 loss 4.58 \n",
            "step  107/200: dist 0.30 loss 3.23 \n",
            "step  108/200: dist 0.30 loss 2.28 \n",
            "step  109/200: dist 0.30 loss 1.72 \n",
            "step  110/200: dist 0.30 loss 2.07 \n",
            "step  111/200: dist 0.30 loss 3.52 \n",
            "step  112/200: dist 0.30 loss 4.36 \n",
            "step  113/200: dist 0.30 loss 3.98 \n",
            "step  114/200: dist 0.30 loss 4.00 \n",
            "step  115/200: dist 0.30 loss 4.57 \n",
            "step  116/200: dist 0.30 loss 3.92 \n",
            "step  117/200: dist 0.29 loss 2.12 \n",
            "step  118/200: dist 0.29 loss 1.13 \n",
            "step  119/200: dist 0.29 loss 1.67 \n",
            "step  120/200: dist 0.29 loss 2.66 \n",
            "step  121/200: dist 0.29 loss 2.83 \n",
            "step  122/200: dist 0.29 loss 1.99 \n",
            "step  123/200: dist 0.29 loss 1.08 \n",
            "step  124/200: dist 0.29 loss 1.12 \n",
            "step  125/200: dist 0.29 loss 2.02 \n",
            "step  126/200: dist 0.29 loss 2.74 \n",
            "step  127/200: dist 0.29 loss 2.79 \n",
            "step  128/200: dist 0.29 loss 2.84 \n",
            "step  129/200: dist 0.29 loss 3.60 \n",
            "step  130/200: dist 0.29 loss 4.42 \n",
            "step  131/200: dist 0.29 loss 3.92 \n",
            "step  132/200: dist 0.29 loss 2.11 \n",
            "step  133/200: dist 0.29 loss 0.73 \n",
            "step  134/200: dist 0.28 loss 1.01 \n",
            "step  135/200: dist 0.28 loss 2.21 \n",
            "step  136/200: dist 0.28 loss 2.87 \n",
            "step  137/200: dist 0.28 loss 2.51 \n",
            "step  138/200: dist 0.28 loss 2.35 \n",
            "step  139/200: dist 0.28 loss 4.12 \n",
            "step  140/200: dist 0.28 loss 8.90 \n",
            "step  141/200: dist 0.28 loss 16.95\n",
            "step  142/200: dist 0.28 loss 25.55\n",
            "step  143/200: dist 0.28 loss 24.43\n",
            "step  144/200: dist 0.28 loss 9.19 \n",
            "step  145/200: dist 0.28 loss 0.88 \n",
            "step  146/200: dist 0.28 loss 10.27\n",
            "step  147/200: dist 0.28 loss 15.24\n",
            "step  148/200: dist 0.28 loss 5.51 \n",
            "step  149/200: dist 0.28 loss 2.30 \n",
            "step  150/200: dist 0.28 loss 9.06 \n",
            "step  151/200: dist 0.28 loss 8.52 \n",
            "step  152/200: dist 0.28 loss 5.43 \n",
            "step  153/200: dist 0.28 loss 9.23 \n",
            "step  154/200: dist 0.28 loss 11.82\n",
            "step  155/200: dist 0.28 loss 11.38\n",
            "step  156/200: dist 0.28 loss 9.62 \n",
            "step  157/200: dist 0.28 loss 3.74 \n",
            "step  158/200: dist 0.28 loss 2.19 \n",
            "step  159/200: dist 0.27 loss 6.40 \n",
            "step  160/200: dist 0.27 loss 4.78 \n",
            "step  161/200: dist 0.27 loss 1.96 \n",
            "step  162/200: dist 0.27 loss 3.06 \n",
            "step  163/200: dist 0.27 loss 2.74 \n",
            "step  164/200: dist 0.27 loss 2.38 \n",
            "step  165/200: dist 0.27 loss 1.50 \n",
            "step  166/200: dist 0.27 loss 1.88 \n",
            "step  167/200: dist 0.27 loss 1.74 \n",
            "step  168/200: dist 0.27 loss 0.77 \n",
            "step  169/200: dist 0.27 loss 1.81 \n",
            "step  170/200: dist 0.27 loss 0.50 \n",
            "step  171/200: dist 0.27 loss 1.34 \n",
            "step  172/200: dist 0.27 loss 0.56 \n",
            "step  173/200: dist 0.27 loss 0.95 \n",
            "step  174/200: dist 0.27 loss 0.54 \n",
            "step  175/200: dist 0.27 loss 0.75 \n",
            "step  176/200: dist 0.27 loss 0.46 \n",
            "step  177/200: dist 0.27 loss 0.64 \n",
            "step  178/200: dist 0.27 loss 0.39 \n",
            "step  179/200: dist 0.27 loss 0.54 \n",
            "step  180/200: dist 0.27 loss 0.38 \n",
            "step  181/200: dist 0.27 loss 0.43 \n",
            "step  182/200: dist 0.27 loss 0.39 \n",
            "step  183/200: dist 0.27 loss 0.34 \n",
            "step  184/200: dist 0.27 loss 0.38 \n",
            "step  185/200: dist 0.27 loss 0.33 \n",
            "step  186/200: dist 0.27 loss 0.31 \n",
            "step  187/200: dist 0.27 loss 0.34 \n",
            "step  188/200: dist 0.27 loss 0.32 \n",
            "step  189/200: dist 0.27 loss 0.29 \n",
            "step  190/200: dist 0.27 loss 0.29 \n",
            "step  191/200: dist 0.27 loss 0.30 \n",
            "step  192/200: dist 0.27 loss 0.30 \n",
            "step  193/200: dist 0.27 loss 0.29 \n",
            "step  194/200: dist 0.27 loss 0.28 \n",
            "step  195/200: dist 0.27 loss 0.27 \n",
            "step  196/200: dist 0.27 loss 0.27 \n",
            "step  197/200: dist 0.27 loss 0.27 \n",
            "step  198/200: dist 0.27 loss 0.27 \n",
            "step  199/200: dist 0.27 loss 0.27 \n",
            "step  200/200: dist 0.27 loss 0.27 \n",
            "Elapsed: 23.2 s\n",
            "Saving optimization progress video \"/content/drive/MyDrive/KHUDA_winter/out0121/proj.mp4\"\n"
          ]
        }
      ],
      "source": [
        "!python projector.py --network=/content/drive/MyDrive/KHUDA_winter/ffhq-res512-mirror-stylegan2-noaug.pkl --outdir=/content/drive/MyDrive/KHUDA_winter/out0121 --target=/content/drive/MyDrive/KHUDA_winter/test_img.png --num-steps=200 --seed=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80YTcjIQARWh",
        "outputId": "6af4918e-cd89-4820-c63e-caab0645c514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/KHUDA_winter/ffhq256-model/ffhq-res256-mirror-paper256-noaug.pkl\"...\n",
            "Computing W midpoint and stddev using 10000 samples...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "step    1/200: dist 0.71 loss 9555.91\n",
            "step    2/200: dist 0.70 loss 13247.63\n",
            "step    3/200: dist 0.62 loss 11506.97\n",
            "step    4/200: dist 0.66 loss 8733.90\n",
            "step    5/200: dist 0.63 loss 5718.08\n",
            "step    6/200: dist 0.60 loss 3206.03\n",
            "step    7/200: dist 0.60 loss 1981.66\n",
            "step    8/200: dist 0.58 loss 2403.15\n",
            "step    9/200: dist 0.56 loss 4000.43\n",
            "step   10/200: dist 0.57 loss 5833.17\n",
            "step   11/200: dist 0.59 loss 6814.40\n",
            "step   12/200: dist 0.58 loss 6572.88\n",
            "step   13/200: dist 0.60 loss 5761.29\n",
            "step   14/200: dist 0.58 loss 4617.29\n",
            "step   15/200: dist 0.56 loss 3444.44\n",
            "step   16/200: dist 0.57 loss 2551.95\n",
            "step   17/200: dist 0.57 loss 2080.47\n",
            "step   18/200: dist 0.52 loss 1776.15\n",
            "step   19/200: dist 0.54 loss 1548.28\n",
            "step   20/200: dist 0.52 loss 1486.68\n",
            "step   21/200: dist 0.54 loss 1577.55\n",
            "step   22/200: dist 0.52 loss 1594.84\n",
            "step   23/200: dist 0.52 loss 1501.50\n",
            "step   24/200: dist 0.50 loss 1397.19\n",
            "step   25/200: dist 0.51 loss 1362.38\n",
            "step   26/200: dist 0.51 loss 1322.67\n",
            "step   27/200: dist 0.50 loss 1191.36\n",
            "step   28/200: dist 0.50 loss 993.54\n",
            "step   29/200: dist 0.49 loss 776.16\n",
            "step   30/200: dist 0.47 loss 584.25\n",
            "step   31/200: dist 0.47 loss 441.21\n",
            "step   32/200: dist 0.48 loss 349.74\n",
            "step   33/200: dist 0.47 loss 328.63\n",
            "step   34/200: dist 0.46 loss 384.61\n",
            "step   35/200: dist 0.45 loss 444.99\n",
            "step   36/200: dist 0.46 loss 488.69\n",
            "step   37/200: dist 0.46 loss 493.65\n",
            "step   38/200: dist 0.46 loss 435.80\n",
            "step   39/200: dist 0.45 loss 341.21\n",
            "step   40/200: dist 0.46 loss 254.84\n",
            "step   41/200: dist 0.46 loss 177.58\n",
            "step   42/200: dist 0.46 loss 124.34\n",
            "step   43/200: dist 0.45 loss 110.16\n",
            "step   44/200: dist 0.46 loss 137.56\n",
            "step   45/200: dist 0.45 loss 172.41\n",
            "step   46/200: dist 0.42 loss 182.44\n",
            "step   47/200: dist 0.43 loss 166.98\n",
            "step   48/200: dist 0.43 loss 139.47\n",
            "step   49/200: dist 0.42 loss 109.02\n",
            "step   50/200: dist 0.42 loss 82.68\n",
            "step   51/200: dist 0.42 loss 72.88\n",
            "step   52/200: dist 0.42 loss 78.56\n",
            "step   53/200: dist 0.41 loss 78.47\n",
            "step   54/200: dist 0.41 loss 67.64\n",
            "step   55/200: dist 0.40 loss 57.82\n",
            "step   56/200: dist 0.40 loss 53.99\n",
            "step   57/200: dist 0.40 loss 48.89\n",
            "step   58/200: dist 0.39 loss 42.08\n",
            "step   59/200: dist 0.39 loss 38.65\n",
            "step   60/200: dist 0.39 loss 34.83\n",
            "step   61/200: dist 0.40 loss 30.49\n",
            "step   62/200: dist 0.39 loss 27.74\n",
            "step   63/200: dist 0.38 loss 29.71\n",
            "step   64/200: dist 0.39 loss 33.08\n",
            "step   65/200: dist 0.40 loss 29.75\n",
            "step   66/200: dist 0.38 loss 22.11\n",
            "step   67/200: dist 0.37 loss 14.86\n",
            "step   68/200: dist 0.37 loss 9.91 \n",
            "step   69/200: dist 0.38 loss 8.55 \n",
            "step   70/200: dist 0.38 loss 10.90\n",
            "step   71/200: dist 0.38 loss 14.39\n",
            "step   72/200: dist 0.37 loss 15.54\n",
            "step   73/200: dist 0.37 loss 14.18\n",
            "step   74/200: dist 0.36 loss 12.62\n",
            "step   75/200: dist 0.36 loss 13.01\n",
            "step   76/200: dist 0.37 loss 14.11\n",
            "step   77/200: dist 0.38 loss 13.96\n",
            "step   78/200: dist 0.37 loss 13.14\n",
            "step   79/200: dist 0.37 loss 11.64\n",
            "step   80/200: dist 0.36 loss 8.19 \n",
            "step   81/200: dist 0.35 loss 5.59 \n",
            "step   82/200: dist 0.35 loss 5.91 \n",
            "step   83/200: dist 0.35 loss 6.92 \n",
            "step   84/200: dist 0.35 loss 6.02 \n",
            "step   85/200: dist 0.35 loss 5.10 \n",
            "step   86/200: dist 0.35 loss 6.31 \n",
            "step   87/200: dist 0.35 loss 6.71 \n",
            "step   88/200: dist 0.35 loss 4.75 \n",
            "step   89/200: dist 0.35 loss 2.95 \n",
            "step   90/200: dist 0.34 loss 3.19 \n",
            "step   91/200: dist 0.34 loss 3.41 \n",
            "step   92/200: dist 0.35 loss 2.13 \n",
            "step   93/200: dist 0.34 loss 1.34 \n",
            "step   94/200: dist 0.34 loss 2.44 \n",
            "step   95/200: dist 0.34 loss 3.51 \n",
            "step   96/200: dist 0.33 loss 2.84 \n",
            "step   97/200: dist 0.33 loss 1.74 \n",
            "step   98/200: dist 0.33 loss 1.78 \n",
            "step   99/200: dist 0.33 loss 2.22 \n",
            "step  100/200: dist 0.33 loss 1.95 \n",
            "step  101/200: dist 0.33 loss 1.46 \n",
            "step  102/200: dist 0.33 loss 1.70 \n",
            "step  103/200: dist 0.33 loss 2.61 \n",
            "step  104/200: dist 0.33 loss 3.54 \n",
            "step  105/200: dist 0.33 loss 4.19 \n",
            "step  106/200: dist 0.33 loss 4.32 \n",
            "step  107/200: dist 0.33 loss 3.51 \n",
            "step  108/200: dist 0.32 loss 2.14 \n",
            "step  109/200: dist 0.33 loss 1.64 \n",
            "step  110/200: dist 0.33 loss 2.65 \n",
            "step  111/200: dist 0.32 loss 4.17 \n",
            "step  112/200: dist 0.32 loss 4.75 \n",
            "step  113/200: dist 0.32 loss 4.03 \n",
            "step  114/200: dist 0.32 loss 2.85 \n",
            "step  115/200: dist 0.32 loss 2.01 \n",
            "step  116/200: dist 0.32 loss 1.67 \n",
            "step  117/200: dist 0.32 loss 1.61 \n",
            "step  118/200: dist 0.32 loss 1.68 \n",
            "step  119/200: dist 0.32 loss 1.93 \n",
            "step  120/200: dist 0.32 loss 2.16 \n",
            "step  121/200: dist 0.31 loss 1.93 \n",
            "step  122/200: dist 0.32 loss 1.34 \n",
            "step  123/200: dist 0.32 loss 0.97 \n",
            "step  124/200: dist 0.31 loss 1.23 \n",
            "step  125/200: dist 0.31 loss 1.89 \n",
            "step  126/200: dist 0.31 loss 2.43 \n",
            "step  127/200: dist 0.31 loss 2.65 \n",
            "step  128/200: dist 0.31 loss 2.95 \n",
            "step  129/200: dist 0.31 loss 3.64 \n",
            "step  130/200: dist 0.31 loss 4.24 \n",
            "step  131/200: dist 0.31 loss 3.79 \n",
            "step  132/200: dist 0.31 loss 2.34 \n",
            "step  133/200: dist 0.31 loss 1.32 \n",
            "step  134/200: dist 0.31 loss 1.68 \n",
            "step  135/200: dist 0.31 loss 2.94 \n",
            "step  136/200: dist 0.31 loss 4.07 \n",
            "step  137/200: dist 0.31 loss 4.74 \n",
            "step  138/200: dist 0.31 loss 5.67 \n",
            "step  139/200: dist 0.30 loss 6.86 \n",
            "step  140/200: dist 0.30 loss 6.68 \n",
            "step  141/200: dist 0.30 loss 4.10 \n",
            "step  142/200: dist 0.30 loss 1.26 \n",
            "step  143/200: dist 0.30 loss 0.88 \n",
            "step  144/200: dist 0.30 loss 2.69 \n",
            "step  145/200: dist 0.30 loss 4.04 \n",
            "step  146/200: dist 0.30 loss 3.15 \n",
            "step  147/200: dist 0.30 loss 1.39 \n",
            "step  148/200: dist 0.30 loss 1.11 \n",
            "step  149/200: dist 0.30 loss 2.34 \n",
            "step  150/200: dist 0.30 loss 3.32 \n",
            "step  151/200: dist 0.30 loss 3.27 \n",
            "step  152/200: dist 0.30 loss 3.57 \n",
            "step  153/200: dist 0.30 loss 5.71 \n",
            "step  154/200: dist 0.30 loss 8.93 \n",
            "step  155/200: dist 0.30 loss 10.15\n",
            "step  156/200: dist 0.30 loss 6.70 \n",
            "step  157/200: dist 0.30 loss 1.71 \n",
            "step  158/200: dist 0.30 loss 1.17 \n",
            "step  159/200: dist 0.30 loss 4.14 \n",
            "step  160/200: dist 0.30 loss 4.14 \n",
            "step  161/200: dist 0.30 loss 1.06 \n",
            "step  162/200: dist 0.30 loss 1.17 \n",
            "step  163/200: dist 0.30 loss 2.89 \n",
            "step  164/200: dist 0.30 loss 1.27 \n",
            "step  165/200: dist 0.30 loss 0.63 \n",
            "step  166/200: dist 0.29 loss 1.93 \n",
            "step  167/200: dist 0.29 loss 0.70 \n",
            "step  168/200: dist 0.29 loss 0.79 \n",
            "step  169/200: dist 0.29 loss 1.23 \n",
            "step  170/200: dist 0.29 loss 0.31 \n",
            "step  171/200: dist 0.29 loss 1.05 \n",
            "step  172/200: dist 0.29 loss 0.37 \n",
            "step  173/200: dist 0.29 loss 0.75 \n",
            "step  174/200: dist 0.29 loss 0.42 \n",
            "step  175/200: dist 0.29 loss 0.59 \n",
            "step  176/200: dist 0.29 loss 0.38 \n",
            "step  177/200: dist 0.29 loss 0.54 \n",
            "step  178/200: dist 0.29 loss 0.31 \n",
            "step  179/200: dist 0.29 loss 0.51 \n",
            "step  180/200: dist 0.29 loss 0.30 \n",
            "step  181/200: dist 0.29 loss 0.41 \n",
            "step  182/200: dist 0.29 loss 0.37 \n",
            "step  183/200: dist 0.29 loss 0.30 \n",
            "step  184/200: dist 0.29 loss 0.38 \n",
            "step  185/200: dist 0.29 loss 0.33 \n",
            "step  186/200: dist 0.29 loss 0.29 \n",
            "step  187/200: dist 0.29 loss 0.33 \n",
            "step  188/200: dist 0.29 loss 0.33 \n",
            "step  189/200: dist 0.29 loss 0.30 \n",
            "step  190/200: dist 0.29 loss 0.29 \n",
            "step  191/200: dist 0.29 loss 0.31 \n",
            "step  192/200: dist 0.29 loss 0.31 \n",
            "step  193/200: dist 0.29 loss 0.30 \n",
            "step  194/200: dist 0.29 loss 0.29 \n",
            "step  195/200: dist 0.29 loss 0.29 \n",
            "step  196/200: dist 0.29 loss 0.29 \n",
            "step  197/200: dist 0.29 loss 0.29 \n",
            "step  198/200: dist 0.29 loss 0.29 \n",
            "step  199/200: dist 0.29 loss 0.29 \n",
            "step  200/200: dist 0.29 loss 0.29 \n",
            "Elapsed: 15.7 s\n",
            "Saving optimization progress video \"/content/drive/MyDrive/KHUDA_winter/out0122/proj.mp4\"\n"
          ]
        }
      ],
      "source": [
        "!python projector.py --network=/content/drive/MyDrive/KHUDA_winter/ffhq256-model/ffhq-res256-mirror-paper256-noaug.pkl --outdir=/content/drive/MyDrive/KHUDA_winter/out0122 --target=/content/drive/MyDrive/KHUDA_winter/test_img.png --num-steps=200 --seed=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import legacy\n",
        "import dnnlib\n",
        "import dnnlib.util\n",
        "import PIL\n",
        "path = \"/content/drive/MyDrive/KHUDA_winter/ffhq256-model/out.pkl\"\n",
        "projected_w = np.load(\"/content/drive/MyDrive/KHUDA_winter/out0121_2/projected_w.npz\")\n",
        "outdir = \"/content/drive/MyDrive/KHUDA_winter/out0122\"\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(path) as fp:\n",
        "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
        "# with open(path, 'rb') as f:\n",
        "#   G = pickle.load(f)['G_ema'].cuda()\n",
        "truncation_psi = 0.7\n",
        "projected_w = projected_w['w']\n",
        "projected_w = torch.from_numpy(projected_w).to('cuda')\n",
        "w_avg = G.mapping.w_avg\n",
        "projected_w = w_avg + (projected_w - w_avg) * truncation_psi\n",
        "synth_image = G.synthesis(projected_w, noise_mode='const')\n",
        "# synth_image = (synth_image + 1) * (255/2)\n",
        "# synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "PIL.Image.fromarray((synth_image*127.5+127).clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype('uint8')).save(f'{outdir}/generate.png')\n",
        "# PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/generate.png')"
      ],
      "metadata": {
        "id": "Eu6U7DyZy2bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAxADbdpHHib"
      },
      "source": [
        "### Peter Baylies’ Projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwS_ey9QF-nk"
      },
      "outputs": [],
      "source": [
        "!python pbaylies_projector.py --help"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffhq_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nahnk2kVMbUP",
        "outputId": "de68fe19-db96-4bd7-e6dd-26b16710fa19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ffhq_dataset (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ffhq_dataset\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = \"/content/drive/MyDrive/KHUDA_winter/test_img/\"\n",
        "aligned = \"/content/drive/MyDrive/KHUDA_winter/test_img/\""
      ],
      "metadata": {
        "id": "BB9H4xYGLETz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python align_images.py $raw $aligned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ6yxz-BKODE",
        "outputId": "025a49a2-4d5c-44bf-d0ee-74579e7a02f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yj06MAABoLe",
        "outputId": "904d81f6-8f13-4555-d24f-fc5188ca9dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/KHUDA_winter/ffhq256-model/ffhq-res256-mirror-paper256-noaug.pkl\"...\n",
            "Computing W midpoint and stddev using 8192 samples...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt ... done\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "step    1/500: dist 12.81 loss 26546.40\n",
            "step    2/500: dist 13.01 loss 12898.91\n",
            "step    3/500: dist 12.46 loss 12829.73\n",
            "step    4/500: dist 13.56 loss 12694.08\n",
            "step    5/500: dist 12.46 loss 12490.50\n",
            "step    6/500: dist 11.94 loss 12225.66\n",
            "step    7/500: dist 12.28 loss 11905.01\n",
            "step    8/500: dist 10.98 loss 11531.78\n",
            "step    9/500: dist 11.15 loss 11114.62\n",
            "step   10/500: dist 10.90 loss 10656.20\n",
            "step   11/500: dist 10.86 loss 10160.95\n",
            "step   12/500: dist 9.82 loss 9631.41\n",
            "step   13/500: dist 9.35 loss 9073.89\n",
            "step   14/500: dist 9.46 loss 8494.35\n",
            "step   15/500: dist 9.45 loss 7899.02\n",
            "step   16/500: dist 8.90 loss 7295.08\n",
            "step   17/500: dist 8.58 loss 6691.00\n",
            "step   18/500: dist 8.38 loss 6093.67\n",
            "step   19/500: dist 7.95 loss 5508.44\n",
            "step   20/500: dist 7.26 loss 4939.59\n",
            "step   21/500: dist 7.18 loss 4391.33\n",
            "step   22/500: dist 7.24 loss 3866.53\n",
            "step   23/500: dist 6.82 loss 3368.46\n",
            "step   24/500: dist 6.85 loss 2901.93\n",
            "step   25/500: dist 6.57 loss 2469.20\n",
            "step   26/500: dist 5.51 loss 2071.75\n",
            "step   27/500: dist 5.84 loss 1713.47\n",
            "step   28/500: dist 5.17 loss 1393.86\n",
            "step   29/500: dist 4.82 loss 1115.62\n",
            "step   30/500: dist 5.00 loss 879.28\n",
            "step   31/500: dist 4.52 loss 683.52\n",
            "step   32/500: dist 4.39 loss 529.53\n",
            "step   33/500: dist 4.11 loss 416.66\n",
            "step   34/500: dist 4.16 loss 343.51\n",
            "step   35/500: dist 4.04 loss 305.39\n",
            "step   36/500: dist 3.86 loss 296.13\n",
            "step   37/500: dist 3.93 loss 308.17\n",
            "step   38/500: dist 4.16 loss 333.07\n",
            "step   39/500: dist 4.21 loss 362.95\n",
            "step   40/500: dist 3.94 loss 391.38\n",
            "step   41/500: dist 4.11 loss 414.17\n",
            "step   42/500: dist 4.01 loss 427.79\n",
            "step   43/500: dist 3.86 loss 431.46\n",
            "step   44/500: dist 3.64 loss 425.33\n",
            "step   45/500: dist 3.77 loss 411.20\n",
            "step   46/500: dist 3.68 loss 390.75\n",
            "step   47/500: dist 3.31 loss 365.70\n",
            "step   48/500: dist 3.41 loss 338.25\n",
            "step   49/500: dist 3.30 loss 309.32\n",
            "step   50/500: dist 3.24 loss 280.34\n",
            "step   51/500: dist 3.30 loss 251.68\n",
            "step   52/500: dist 3.04 loss 222.45\n",
            "step   53/500: dist 3.17 loss 193.11\n",
            "step   54/500: dist 2.95 loss 163.84\n",
            "step   55/500: dist 3.00 loss 136.92\n",
            "step   56/500: dist 3.06 loss 113.90\n",
            "step   57/500: dist 2.93 loss 95.57\n",
            "step   58/500: dist 2.88 loss 81.94\n",
            "step   59/500: dist 2.76 loss 72.47\n",
            "step   60/500: dist 2.56 loss 66.29\n",
            "step   61/500: dist 2.67 loss 62.23\n",
            "step   62/500: dist 2.48 loss 58.72\n",
            "step   63/500: dist 2.66 loss 56.29\n",
            "step   64/500: dist 2.75 loss 54.00\n",
            "step   65/500: dist 2.53 loss 51.19\n",
            "step   66/500: dist 2.40 loss 48.68\n",
            "step   67/500: dist 2.30 loss 46.66\n",
            "step   68/500: dist 2.29 loss 45.04\n",
            "step   69/500: dist 2.29 loss 43.54\n",
            "step   70/500: dist 2.24 loss 41.57\n",
            "step   71/500: dist 2.42 loss 38.82\n",
            "step   72/500: dist 2.19 loss 34.87\n",
            "step   73/500: dist 2.09 loss 30.70\n",
            "step   74/500: dist 2.22 loss 26.77\n",
            "step   75/500: dist 1.95 loss 22.86\n",
            "step   76/500: dist 2.09 loss 20.05\n",
            "step   77/500: dist 2.08 loss 17.80\n",
            "step   78/500: dist 1.90 loss 15.97\n",
            "step   79/500: dist 1.84 loss 14.81\n",
            "step   80/500: dist 1.76 loss 14.17\n",
            "step   81/500: dist 1.62 loss 12.96\n",
            "step   82/500: dist 1.98 loss 11.11\n",
            "step   83/500: dist 1.69 loss 10.07\n",
            "step   84/500: dist 1.63 loss 10.36\n",
            "step   85/500: dist 1.89 loss 9.64 \n",
            "step   86/500: dist 1.64 loss 8.89 \n",
            "step   87/500: dist 1.51 loss 9.04 \n",
            "step   88/500: dist 1.70 loss 8.31 \n",
            "step   89/500: dist 1.85 loss 7.90 \n",
            "step   90/500: dist 1.86 loss 8.10 \n",
            "step   91/500: dist 2.03 loss 7.88 \n",
            "step   92/500: dist 1.63 loss 7.57 \n",
            "step   93/500: dist 1.32 loss 7.06 \n",
            "step   94/500: dist 1.51 loss 5.46 \n",
            "step   95/500: dist 1.54 loss 4.74 \n",
            "step   96/500: dist 1.35 loss 4.91 \n",
            "step   97/500: dist 1.67 loss 4.46 \n",
            "step   98/500: dist 1.54 loss 4.01 \n",
            "step   99/500: dist 1.28 loss 4.87 \n",
            "step  100/500: dist 1.24 loss 4.26 \n",
            "step  101/500: dist 1.39 loss 3.20 \n",
            "step  102/500: dist 1.38 loss 4.58 \n",
            "step  103/500: dist 1.21 loss 5.33 \n",
            "step  104/500: dist 1.23 loss 3.57 \n",
            "step  105/500: dist 0.93 loss 2.75 \n",
            "step  106/500: dist 1.22 loss 4.22 \n",
            "step  107/500: dist 1.22 loss 3.76 \n",
            "step  108/500: dist 1.10 loss 2.48 \n",
            "step  109/500: dist 1.02 loss 2.63 \n",
            "step  110/500: dist 1.16 loss 2.94 \n",
            "step  111/500: dist 1.00 loss 2.04 \n",
            "step  112/500: dist 1.00 loss 1.79 \n",
            "step  113/500: dist 1.00 loss 2.15 \n",
            "step  114/500: dist 1.09 loss 2.15 \n",
            "step  115/500: dist 1.05 loss 1.76 \n",
            "step  116/500: dist 1.14 loss 1.82 \n",
            "step  117/500: dist 0.99 loss 1.79 \n",
            "step  118/500: dist 0.84 loss 1.63 \n",
            "step  119/500: dist 1.12 loss 1.90 \n",
            "step  120/500: dist 0.92 loss 1.83 \n",
            "step  121/500: dist 0.89 loss 1.86 \n",
            "step  122/500: dist 0.87 loss 1.77 \n",
            "step  123/500: dist 1.16 loss 2.17 \n",
            "step  124/500: dist 0.89 loss 2.46 \n",
            "step  125/500: dist 0.69 loss 3.24 \n",
            "step  126/500: dist 1.02 loss 4.35 \n",
            "step  127/500: dist 0.71 loss 3.52 \n",
            "step  128/500: dist 0.73 loss 1.95 \n",
            "step  129/500: dist 0.83 loss 1.46 \n",
            "step  130/500: dist 1.01 loss 2.72 \n",
            "step  131/500: dist 1.04 loss 3.83 \n",
            "step  132/500: dist 0.87 loss 3.29 \n",
            "step  133/500: dist 0.91 loss 2.35 \n",
            "step  134/500: dist 0.79 loss 2.22 \n",
            "step  135/500: dist 1.26 loss 3.35 \n",
            "step  136/500: dist 1.20 loss 3.23 \n",
            "step  137/500: dist 0.90 loss 2.07 \n",
            "step  138/500: dist 0.56 loss 1.53 \n",
            "step  139/500: dist 0.83 loss 3.40 \n",
            "step  140/500: dist 0.82 loss 6.86 \n",
            "step  141/500: dist 0.67 loss 11.19\n",
            "step  142/500: dist 0.86 loss 12.95\n",
            "step  143/500: dist 1.25 loss 9.38 \n",
            "step  144/500: dist 0.80 loss 7.96 \n",
            "step  145/500: dist 0.65 loss 12.15\n",
            "step  146/500: dist 1.01 loss 11.44\n",
            "step  147/500: dist 0.74 loss 7.64 \n",
            "step  148/500: dist 0.51 loss 9.23 \n",
            "step  149/500: dist 0.77 loss 8.54 \n",
            "step  150/500: dist 0.94 loss 4.99 \n",
            "step  151/500: dist 0.76 loss 8.70 \n",
            "step  152/500: dist 0.50 loss 15.27\n",
            "step  153/500: dist 0.90 loss 12.34\n",
            "step  154/500: dist 0.86 loss 5.55 \n",
            "step  155/500: dist 0.53 loss 8.58 \n",
            "step  156/500: dist 0.56 loss 8.39 \n",
            "step  157/500: dist 0.57 loss 3.39 \n",
            "step  158/500: dist 0.58 loss 6.67 \n",
            "step  159/500: dist 0.79 loss 6.43 \n",
            "step  160/500: dist 0.57 loss 2.84 \n",
            "step  161/500: dist 0.51 loss 6.11 \n",
            "step  162/500: dist 0.54 loss 5.02 \n",
            "step  163/500: dist 0.57 loss 3.85 \n",
            "step  164/500: dist 0.51 loss 6.11 \n",
            "step  165/500: dist 0.76 loss 3.72 \n",
            "step  166/500: dist 0.35 loss 2.46 \n",
            "step  167/500: dist 0.55 loss 3.93 \n",
            "step  168/500: dist 0.75 loss 2.67 \n",
            "step  169/500: dist 0.62 loss 2.79 \n",
            "step  170/500: dist 0.33 loss 2.44 \n",
            "step  171/500: dist 0.53 loss 1.56 \n",
            "step  172/500: dist 0.61 loss 2.56 \n",
            "step  173/500: dist 0.31 loss 2.21 \n",
            "step  174/500: dist 0.58 loss 2.45 \n",
            "step  175/500: dist 0.39 loss 4.17 \n",
            "step  176/500: dist 0.34 loss 5.09 \n",
            "step  177/500: dist 0.69 loss 6.02 \n",
            "step  178/500: dist 0.31 loss 6.48 \n",
            "step  179/500: dist 0.37 loss 5.12 \n",
            "step  180/500: dist 0.74 loss 3.05 \n",
            "step  181/500: dist 0.34 loss 1.89 \n",
            "step  182/500: dist 0.39 loss 2.62 \n",
            "step  183/500: dist 0.29 loss 4.24 \n",
            "step  184/500: dist 0.37 loss 6.46 \n",
            "step  185/500: dist 0.26 loss 7.63 \n",
            "step  186/500: dist 0.26 loss 8.80 \n",
            "step  187/500: dist 0.30 loss 10.30\n",
            "step  188/500: dist 0.33 loss 9.82 \n",
            "step  189/500: dist 0.26 loss 7.88 \n",
            "step  190/500: dist 0.28 loss 9.25 \n",
            "step  191/500: dist 0.21 loss 13.14\n",
            "step  192/500: dist 0.24 loss 13.51\n",
            "step  193/500: dist 0.33 loss 8.60 \n",
            "step  194/500: dist 0.22 loss 5.96 \n",
            "step  195/500: dist 0.56 loss 13.77\n",
            "step  196/500: dist 0.20 loss 29.76\n",
            "step  197/500: dist 0.28 loss 39.23\n",
            "step  198/500: dist 0.26 loss 18.80\n",
            "step  199/500: dist 0.24 loss 4.57 \n",
            "step  200/500: dist 0.24 loss 21.92\n",
            "step  201/500: dist 0.21 loss 17.81\n",
            "step  202/500: dist 0.18 loss 10.95\n",
            "step  203/500: dist 0.27 loss 22.39\n",
            "step  204/500: dist 0.20 loss 13.61\n",
            "step  205/500: dist 0.45 loss 18.10\n",
            "step  206/500: dist 0.19 loss 21.26\n",
            "step  207/500: dist 0.13 loss 8.12 \n",
            "step  208/500: dist 0.60 loss 17.20\n",
            "step  209/500: dist 0.16 loss 19.61\n",
            "step  210/500: dist 0.59 loss 24.62\n",
            "step  211/500: dist 0.18 loss 32.94\n",
            "step  212/500: dist 0.27 loss 35.25\n",
            "step  213/500: dist 0.15 loss 39.88\n",
            "step  214/500: dist 0.22 loss 21.42\n",
            "step  215/500: dist 0.30 loss 16.30\n",
            "step  216/500: dist 0.38 loss 28.62\n",
            "step  217/500: dist 0.20 loss 18.20\n",
            "step  218/500: dist 0.19 loss 12.70\n",
            "step  219/500: dist 0.98 loss 20.76\n",
            "step  220/500: dist 0.10 loss 17.83\n",
            "step  221/500: dist 0.60 loss 11.31\n",
            "step  222/500: dist 0.31 loss 13.33\n",
            "step  223/500: dist 0.26 loss 13.31\n",
            "step  224/500: dist 0.39 loss 5.20 \n",
            "step  225/500: dist 0.12 loss 9.57 \n",
            "step  226/500: dist 0.52 loss 15.22\n",
            "step  227/500: dist 0.25 loss 12.86\n",
            "step  228/500: dist 0.25 loss 22.12\n",
            "step  229/500: dist 0.11 loss 30.62\n",
            "step  230/500: dist 0.52 loss 31.29\n",
            "step  231/500: dist 0.12 loss 25.65\n",
            "step  232/500: dist 0.35 loss 12.57\n",
            "step  233/500: dist 0.15 loss 8.66 \n",
            "step  234/500: dist 0.10 loss 18.35\n",
            "step  235/500: dist 0.30 loss 17.33\n",
            "step  236/500: dist 0.09 loss 6.82 \n",
            "step  237/500: dist 0.20 loss 7.03 \n",
            "step  238/500: dist 0.09 loss 11.91\n",
            "step  239/500: dist 0.49 loss 9.95 \n",
            "step  240/500: dist 0.08 loss 5.10 \n",
            "step  241/500: dist 0.71 loss 6.24 \n",
            "step  242/500: dist 0.07 loss 7.39 \n",
            "step  243/500: dist 0.68 loss 7.19 \n",
            "step  244/500: dist 0.09 loss 5.69 \n",
            "step  245/500: dist 0.61 loss 7.98 \n",
            "step  246/500: dist 0.10 loss 11.96\n",
            "step  247/500: dist 0.61 loss 19.95\n",
            "step  248/500: dist 0.05 loss 31.28\n",
            "step  249/500: dist 0.37 loss 42.94\n",
            "step  250/500: dist 0.06 loss 41.14\n",
            "step  251/500: dist 0.19 loss 39.13\n",
            "step  252/500: dist 0.04 loss 52.31\n",
            "step  253/500: dist 0.35 loss 50.87\n",
            "step  254/500: dist 0.03 loss 16.43\n",
            "step  255/500: dist 0.46 loss 8.24 \n",
            "step  256/500: dist 0.02 loss 32.95\n",
            "step  257/500: dist 0.23 loss 29.90\n",
            "step  258/500: dist 0.01 loss 11.85\n",
            "step  259/500: dist 0.12 loss 23.03\n",
            "step  260/500: dist 0.02 loss 35.85\n",
            "step  261/500: dist 0.05 loss 36.71\n",
            "step  262/500: dist 0.19 loss 30.44\n",
            "step  263/500: dist 0.01 loss 14.03\n",
            "step  264/500: dist 0.36 loss 12.01\n",
            "step  265/500: dist 0.00 loss 19.43\n",
            "step  266/500: dist 0.26 loss 15.49\n",
            "step  267/500: dist 0.02 loss 9.77 \n",
            "step  268/500: dist 0.05 loss 10.15\n",
            "step  269/500: dist 0.00 loss 11.54\n",
            "step  270/500: dist 0.03 loss 10.14\n",
            "step  271/500: dist 0.17 loss 7.35 \n",
            "step  272/500: dist 0.00 loss 10.75\n",
            "step  273/500: dist 0.34 loss 10.23\n",
            "step  274/500: dist 0.00 loss 5.38 \n",
            "step  275/500: dist 1.04 loss 10.34\n",
            "step  276/500: dist 0.03 loss 7.21 \n",
            "step  277/500: dist 1.92 loss 4.45 \n",
            "step  278/500: dist 0.05 loss 5.91 \n",
            "step  279/500: dist 1.44 loss 4.83 \n",
            "step  280/500: dist 0.21 loss 1.08 \n",
            "step  281/500: dist 0.50 loss 4.83 \n",
            "step  282/500: dist 0.70 loss 3.64 \n",
            "step  283/500: dist 0.05 loss 2.03 \n",
            "step  284/500: dist 0.90 loss 4.71 \n",
            "step  285/500: dist 0.23 loss 2.80 \n",
            "step  286/500: dist 0.20 loss 2.22 \n",
            "step  287/500: dist 0.45 loss 2.87 \n",
            "step  288/500: dist 0.07 loss 1.58 \n",
            "step  289/500: dist 0.46 loss 1.84 \n",
            "step  290/500: dist 0.05 loss 1.57 \n",
            "step  291/500: dist 0.31 loss 1.57 \n",
            "step  292/500: dist 0.10 loss 1.57 \n",
            "step  293/500: dist 0.09 loss 1.70 \n",
            "step  294/500: dist 0.19 loss 1.50 \n",
            "step  295/500: dist 0.02 loss 1.24 \n",
            "step  296/500: dist 0.53 loss 1.82 \n",
            "step  297/500: dist 0.02 loss 0.97 \n",
            "step  298/500: dist 0.21 loss 0.95 \n",
            "step  299/500: dist 0.04 loss 1.03 \n",
            "step  300/500: dist 0.05 loss 0.96 \n",
            "step  301/500: dist 0.29 loss 1.13 \n",
            "step  302/500: dist 0.00 loss 1.12 \n",
            "step  303/500: dist 0.32 loss 1.55 \n",
            "step  304/500: dist 0.00 loss 1.42 \n",
            "step  305/500: dist 0.03 loss 1.77 \n",
            "step  306/500: dist 0.05 loss 2.01 \n",
            "step  307/500: dist 0.00 loss 2.40 \n",
            "step  308/500: dist 0.16 loss 3.07 \n",
            "step  309/500: dist 0.00 loss 3.24 \n",
            "step  310/500: dist 0.00 loss 3.81 \n",
            "step  311/500: dist 0.07 loss 4.57 \n",
            "step  312/500: dist 0.00 loss 5.14 \n",
            "step  313/500: dist 0.00 loss 6.01 \n",
            "step  314/500: dist 0.27 loss 6.99 \n",
            "step  315/500: dist 0.00 loss 6.70 \n",
            "step  316/500: dist 0.26 loss 5.69 \n",
            "step  317/500: dist 0.00 loss 3.06 \n",
            "step  318/500: dist 0.00 loss 0.93 \n",
            "step  319/500: dist 0.01 loss 0.54 \n",
            "step  320/500: dist 0.02 loss 1.72 \n",
            "step  321/500: dist 0.00 loss 3.11 \n",
            "step  322/500: dist 0.00 loss 3.49 \n",
            "step  323/500: dist 0.00 loss 2.46 \n",
            "step  324/500: dist 0.00 loss 1.04 \n",
            "step  325/500: dist 0.02 loss 0.56 \n",
            "step  326/500: dist 0.00 loss 1.12 \n",
            "step  327/500: dist 0.00 loss 1.98 \n",
            "step  328/500: dist 0.00 loss 2.32 \n",
            "step  329/500: dist 0.00 loss 2.25 \n",
            "step  330/500: dist 0.00 loss 2.56 \n",
            "step  331/500: dist 0.02 loss 3.91 \n",
            "step  332/500: dist 0.00 loss 5.88 \n",
            "step  333/500: dist 0.00 loss 7.58 \n",
            "step  334/500: dist 0.00 loss 7.79 \n",
            "step  335/500: dist 0.00 loss 6.12 \n",
            "step  336/500: dist 0.00 loss 3.24 \n",
            "step  337/500: dist 0.00 loss 0.99 \n",
            "step  338/500: dist 0.01 loss 0.68 \n",
            "step  339/500: dist 0.00 loss 1.87 \n",
            "step  340/500: dist 0.00 loss 3.05 \n",
            "step  341/500: dist 0.00 loss 2.94 \n",
            "step  342/500: dist 0.00 loss 1.69 \n",
            "step  343/500: dist 0.00 loss 0.61 \n",
            "step  344/500: dist 0.00 loss 0.77 \n",
            "step  345/500: dist 0.00 loss 1.86 \n",
            "step  346/500: dist 0.00 loss 2.83 \n",
            "step  347/500: dist 0.02 loss 3.29 \n",
            "step  348/500: dist 0.00 loss 3.98 \n",
            "step  349/500: dist 0.45 loss 6.57 \n",
            "step  350/500: dist 0.00 loss 9.49 \n",
            "step  351/500: dist 0.13 loss 12.35\n",
            "step  352/500: dist 0.00 loss 11.18\n",
            "step  353/500: dist 0.02 loss 6.13 \n",
            "step  354/500: dist 0.04 loss 1.14 \n",
            "step  355/500: dist 0.00 loss 0.77 \n",
            "step  356/500: dist 0.31 loss 4.48 \n",
            "step  357/500: dist 0.00 loss 6.34 \n",
            "step  358/500: dist 0.00 loss 4.59 \n",
            "step  359/500: dist 0.24 loss 1.97 \n",
            "step  360/500: dist 0.00 loss 2.13 \n",
            "step  361/500: dist 0.27 loss 5.60 \n",
            "step  362/500: dist 0.00 loss 7.47 \n",
            "step  363/500: dist 0.00 loss 7.35 \n",
            "step  364/500: dist 0.32 loss 7.63 \n",
            "step  365/500: dist 0.00 loss 8.42 \n",
            "step  366/500: dist 0.13 loss 8.07 \n",
            "step  367/500: dist 0.00 loss 4.42 \n",
            "step  368/500: dist 0.00 loss 0.79 \n",
            "step  369/500: dist 0.00 loss 0.48 \n",
            "step  370/500: dist 0.18 loss 2.90 \n",
            "step  371/500: dist 0.00 loss 4.14 \n",
            "step  372/500: dist 0.28 loss 3.61 \n",
            "step  373/500: dist 0.00 loss 1.68 \n",
            "step  374/500: dist 0.00 loss 0.87 \n",
            "step  375/500: dist 0.11 loss 1.15 \n",
            "step  376/500: dist 0.00 loss 1.58 \n",
            "step  377/500: dist 0.12 loss 2.11 \n",
            "step  378/500: dist 0.00 loss 2.01 \n",
            "step  379/500: dist 0.00 loss 1.75 \n",
            "step  380/500: dist 0.26 loss 2.31 \n",
            "step  381/500: dist 0.00 loss 3.79 \n",
            "step  382/500: dist 0.02 loss 7.06 \n",
            "step  383/500: dist 0.00 loss 10.90\n",
            "step  384/500: dist 0.01 loss 13.92\n",
            "step  385/500: dist 0.00 loss 13.04\n",
            "step  386/500: dist 0.00 loss 7.38 \n",
            "step  387/500: dist 0.19 loss 1.79 \n",
            "step  388/500: dist 0.00 loss 1.66 \n",
            "step  389/500: dist 0.17 loss 5.79 \n",
            "step  390/500: dist 0.00 loss 6.62 \n",
            "step  391/500: dist 0.03 loss 2.98 \n",
            "step  392/500: dist 0.16 loss 0.62 \n",
            "step  393/500: dist 0.00 loss 2.49 \n",
            "step  394/500: dist 0.02 loss 4.36 \n",
            "step  395/500: dist 0.00 loss 2.31 \n",
            "step  396/500: dist 0.03 loss 0.18 \n",
            "step  397/500: dist 0.13 loss 1.52 \n",
            "step  398/500: dist 0.00 loss 2.86 \n",
            "step  399/500: dist 0.22 loss 1.77 \n",
            "step  400/500: dist 0.00 loss 0.09 \n",
            "step  401/500: dist 0.05 loss 1.01 \n",
            "step  402/500: dist 0.20 loss 2.02 \n",
            "step  403/500: dist 0.00 loss 0.85 \n",
            "step  404/500: dist 0.04 loss 0.16 \n",
            "step  405/500: dist 0.37 loss 1.22 \n",
            "step  406/500: dist 0.00 loss 1.15 \n",
            "step  407/500: dist 0.53 loss 0.88 \n",
            "step  408/500: dist 0.02 loss 0.12 \n",
            "step  409/500: dist 0.00 loss 0.72 \n",
            "step  410/500: dist 0.09 loss 0.87 \n",
            "step  411/500: dist 0.00 loss 0.20 \n",
            "step  412/500: dist 0.00 loss 0.18 \n",
            "step  413/500: dist 0.25 loss 0.86 \n",
            "step  414/500: dist 0.00 loss 0.62 \n",
            "step  415/500: dist 0.33 loss 0.78 \n",
            "step  416/500: dist 0.00 loss 0.84 \n",
            "step  417/500: dist 0.00 loss 1.61 \n",
            "step  418/500: dist 0.53 loss 3.05 \n",
            "step  419/500: dist 0.00 loss 4.31 \n",
            "step  420/500: dist 0.45 loss 8.49 \n",
            "step  421/500: dist 0.00 loss 14.22\n",
            "step  422/500: dist 0.02 loss 22.71\n",
            "step  423/500: dist 0.07 loss 31.38\n",
            "step  424/500: dist 0.00 loss 35.29\n",
            "step  425/500: dist 0.00 loss 32.05\n",
            "step  426/500: dist 0.11 loss 26.10\n",
            "step  427/500: dist 0.00 loss 21.49\n",
            "step  428/500: dist 0.00 loss 20.49\n",
            "step  429/500: dist 0.03 loss 23.89\n",
            "step  430/500: dist 0.11 loss 30.09\n",
            "step  431/500: dist 0.00 loss 31.31\n",
            "step  432/500: dist 0.02 loss 19.31\n",
            "step  433/500: dist 0.36 loss 6.13 \n",
            "step  434/500: dist 0.00 loss 7.33 \n",
            "step  435/500: dist 0.12 loss 17.44\n",
            "step  436/500: dist 0.00 loss 15.93\n",
            "step  437/500: dist 0.00 loss 5.48 \n",
            "step  438/500: dist 0.00 loss 5.31 \n",
            "step  439/500: dist 0.17 loss 13.47\n",
            "step  440/500: dist 0.00 loss 14.23\n",
            "step  441/500: dist 0.25 loss 12.74\n",
            "step  442/500: dist 0.00 loss 18.55\n",
            "step  443/500: dist 0.00 loss 25.65\n",
            "step  444/500: dist 0.03 loss 25.07\n",
            "step  445/500: dist 0.10 loss 18.22\n",
            "step  446/500: dist 0.00 loss 8.95 \n",
            "step  447/500: dist 0.00 loss 4.85 \n",
            "step  448/500: dist 0.68 loss 10.21\n",
            "step  449/500: dist 0.00 loss 13.41\n",
            "step  450/500: dist 0.01 loss 7.62 \n",
            "step  451/500: dist 0.00 loss 2.27 \n",
            "step  452/500: dist 0.14 loss 6.53 \n",
            "step  453/500: dist 0.00 loss 9.36 \n",
            "step  454/500: dist 0.04 loss 5.03 \n",
            "step  455/500: dist 0.00 loss 3.63 \n",
            "step  456/500: dist 0.00 loss 7.61 \n",
            "step  457/500: dist 0.00 loss 6.60 \n",
            "step  458/500: dist 0.00 loss 1.79 \n",
            "step  459/500: dist 0.15 loss 2.26 \n",
            "step  460/500: dist 0.00 loss 3.93 \n",
            "step  461/500: dist 0.00 loss 2.21 \n",
            "step  462/500: dist 0.41 loss 2.03 \n",
            "step  463/500: dist 0.00 loss 2.07 \n",
            "step  464/500: dist 0.01 loss 1.14 \n",
            "step  465/500: dist 0.10 loss 1.56 \n",
            "step  466/500: dist 0.00 loss 1.12 \n",
            "step  467/500: dist 0.00 loss 0.65 \n",
            "step  468/500: dist 0.05 loss 1.21 \n",
            "step  469/500: dist 0.11 loss 0.45 \n",
            "step  470/500: dist 0.00 loss 0.82 \n",
            "step  471/500: dist 0.00 loss 0.35 \n",
            "step  472/500: dist 0.01 loss 0.51 \n",
            "step  473/500: dist 0.04 loss 0.35 \n",
            "step  474/500: dist 0.01 loss 0.36 \n",
            "step  475/500: dist 0.00 loss 0.22 \n",
            "step  476/500: dist 0.00 loss 0.29 \n",
            "step  477/500: dist 0.00 loss 0.12 \n",
            "step  478/500: dist 0.00 loss 0.24 \n",
            "step  479/500: dist 0.00 loss 0.08 \n",
            "step  480/500: dist 0.00 loss 0.15 \n",
            "step  481/500: dist 0.00 loss 0.11 \n",
            "step  482/500: dist 0.00 loss 0.06 \n",
            "step  483/500: dist 0.00 loss 0.09 \n",
            "step  484/500: dist 0.00 loss 0.06 \n",
            "step  485/500: dist 0.01 loss 0.04 \n",
            "step  486/500: dist 0.00 loss 0.05 \n",
            "step  487/500: dist 0.00 loss 0.05 \n",
            "step  488/500: dist 0.00 loss 0.03 \n",
            "step  489/500: dist 0.00 loss 0.02 \n",
            "step  490/500: dist 0.00 loss 0.02 \n",
            "step  491/500: dist 0.00 loss 0.02 \n",
            "step  492/500: dist 0.00 loss 0.02 \n",
            "step  493/500: dist 0.00 loss 0.01 \n",
            "step  494/500: dist 0.00 loss 0.01 \n",
            "step  495/500: dist 0.00 loss 0.01 \n",
            "step  496/500: dist 0.00 loss 0.00 \n",
            "step  497/500: dist 0.00 loss 0.00 \n",
            "step  498/500: dist 0.00 loss 0.00 \n",
            "step  499/500: dist 0.00 loss 0.00 \n",
            "step  500/500: dist 0.00 loss 0.00 \n",
            "Elapsed: 65.8 s\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/plugins/ffmpeg.py\", line 59, in _get_ffmpeg_api\n",
            "    import imageio_ffmpeg\n",
            "ModuleNotFoundError: No module named 'imageio_ffmpeg'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"pbaylies_projector.py\", line 398, in <module>\n",
            "    run_projection() # pylint: disable=no-value-for-parameter\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"pbaylies_projector.py\", line 383, in run_projection\n",
            "    video = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/core/functions.py\", line 231, in get_writer\n",
            "    return format.get_writer(request)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/core/format.py\", line 185, in get_writer\n",
            "    return self.Writer(self, request)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/core/format.py\", line 221, in __init__\n",
            "    self._open(**self.request.kwargs.copy())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/plugins/ffmpeg.py\", line 525, in _open\n",
            "    self._ffmpeg_api = _get_ffmpeg_api()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/imageio/plugins/ffmpeg.py\", line 61, in _get_ffmpeg_api\n",
            "    raise ImportError(\n",
            "ImportError: To use the imageio ffmpeg plugin you need to 'pip install imageio-ffmpeg'\n"
          ]
        }
      ],
      "source": [
        "!python pbaylies_projector.py --network=/content/drive/MyDrive/KHUDA_winter/ffhq256-model/ffhq-res256-mirror-paper256-noaug.pkl --outdir=/content/drive/MyDrive/KHUDA_winter/out0122 --target-image=/content/drive/MyDrive/KHUDA_winter/test_img/test_img2_01.png --num-steps=500 --use-clip=False --use-center=False --seed=99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qywlaS5pgzyH"
      },
      "source": [
        "## Combine NPZ files together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2VooqrNfIpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c09e8d6-f52e-4cbf-ffd5-0b1a749c768a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combining .npz files...\n",
            "/content/drive/MyDrive/KHUDA_winter/out0121_2/projected_w.npz\n",
            "torch.Size([1, 14, 512])\n"
          ]
        }
      ],
      "source": [
        "!python combine_npz.py --outdir=/content/npz --npzs='/content/drive/MyDrive/KHUDA_winter/out0121_2/projected_w.npz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIqgl5nIHwpp",
        "outputId": "3c80caac-f120-4e6e-91dc-06355c7628b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "Usage: generate.py [OPTIONS]\n",
            "\n",
            "  Generate images using pretrained network\n",
            "  pickle.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  # Generate curated MetFaces images without truncation (Fig.10 left)\n",
            "  python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "  # Generate uncurated MetFaces images with truncation (Fig.12 upper left)\n",
            "  python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "  # Generate class conditional CIFAR-10 images (Fig.17 left, Car)\n",
            "  python generate.py --outdir=out --seeds=0-35 --class=1 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n",
            "\n",
            "  # Render an image from projected W\n",
            "  python generate.py --outdir=out --projected_w=projected_w.npz \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "Options:\n",
            "  --network TEXT                  Network pickle\n",
            "                                  filename\n",
            "                                  [required]\n",
            "\n",
            "  --seeds NUM_RANGE               List of random\n",
            "                                  seeds\n",
            "\n",
            "  --trunc FLOAT                   Truncation psi\n",
            "                                  [default: 1]\n",
            "\n",
            "  --class INTEGER                 Class label\n",
            "                                  (unconditional\n",
            "                                  if not\n",
            "                                  specified)\n",
            "\n",
            "  --diameter FLOAT                diameter of\n",
            "                                  loops  [default:\n",
            "                                  100.0]\n",
            "\n",
            "  --frames INTEGER                how many frames\n",
            "                                  to produce (with\n",
            "                                  seeds this is\n",
            "                                  frames between\n",
            "                                  each step, with\n",
            "                                  loops this is\n",
            "                                  total length)\n",
            "                                  [default: 240]\n",
            "\n",
            "  --fps INTEGER                   framerate for\n",
            "                                  video  [default:\n",
            "                                  24]\n",
            "\n",
            "  --increment FLOAT               truncation\n",
            "                                  increment value\n",
            "                                  [default: 0.01]\n",
            "\n",
            "  --interpolation [linear|slerp|noiseloop|circularloop]\n",
            "                                  interpolation\n",
            "                                  type  [required]\n",
            "\n",
            "  --easing [linear|easeInOutQuad|bounceEaseOut|circularEaseOut|circularEaseOut2]\n",
            "                                  easing method\n",
            "                                  [required]\n",
            "\n",
            "  --network TEXT                  Network pickle\n",
            "                                  filename\n",
            "                                  [required]\n",
            "\n",
            "  --noise-mode [const|random|none]\n",
            "                                  Noise mode\n",
            "                                  [default: const]\n",
            "\n",
            "  --outdir DIR                    Where to save\n",
            "                                  the output\n",
            "                                  images\n",
            "                                  [required]\n",
            "\n",
            "  --process [image|interpolation|truncation|interpolation-truncation]\n",
            "                                  generation\n",
            "                                  method\n",
            "                                  [required]\n",
            "\n",
            "  --projected-w FILE              Projection\n",
            "                                  result file\n",
            "\n",
            "  --random_seed INTEGER           random seed\n",
            "                                  value (used in\n",
            "                                  noise and\n",
            "                                  circular loop)\n",
            "                                  [default: 0]\n",
            "\n",
            "  --scale-type [pad|padside|symm|symmside]\n",
            "                                  scaling method\n",
            "                                  for --size\n",
            "\n",
            "  --size SIZE_RANGE               size of output\n",
            "                                  (in format x-y)\n",
            "\n",
            "  --seeds NUM_RANGE               List of random\n",
            "                                  seeds\n",
            "\n",
            "  --space [z|w]                   latent space\n",
            "                                  [required]\n",
            "\n",
            "  --start FLOAT                   starting\n",
            "                                  truncation value\n",
            "                                  [default: 0.0]\n",
            "\n",
            "  --stop FLOAT                    stopping\n",
            "                                  truncation value\n",
            "                                  [default: 1.0]\n",
            "\n",
            "  --trunc FLOAT                   Truncation psi\n",
            "                                  [default: 1]\n",
            "\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --help"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "latent_dir = Path(\"/content/drive/MyDrive/KHUDA_winter/out0121\")\n",
        "latents = latent_dir.glob(\"*.npz\")\n",
        "for latent_file in latents:\n",
        "  latent = np.load(latent_file)\n",
        "  latent = np.expand_dims(latent,axis=0)\n",
        "  synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=False), minibatch_size=8)\n",
        "  images = Gs_blended.components.synthesis.run(latent, randomize_noise=False, **synthesis_kwargs)\n",
        "  Image.fromarray(images.transpose((0,2,3,1))[0], 'RGB').save(latent_file.parent / (f\"{latent_file.stem}-toon.jpg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "BmMhxqvU5GGX",
        "outputId": "bf5e5f9f-ec12-4d2a-c24e-6fff9c959d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-d8de65604e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtflib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnnlib.tflib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R_EgaBNvVit",
        "outputId": "106dbf30-1e82-4a8d-b354-d2247a394473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.1\n",
            "    Uninstalling numpy-1.24.1:\n",
            "      Successfully uninstalled numpy-1.24.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cgezYN8Dsyh",
        "outputId": "6427a780-a212-49f0-88e0-4d252566315c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate.py:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  elif(len(seeds) is not 3):\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Loading networks from \"/content/drive/MyDrive/KHUDA_winter/ffhq256-model/out.pkl\"...\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "\u001b[0;35m[image2 @ 0x555b72ede000] \u001b[0m\u001b[1;31mCould find no file with path '/content/drive/MyDrive/KHUDA_winter/out0121_2/frames/frame%04d.png' and index in the range 0-4\n",
            "\u001b[0m\u001b[1;31m/content/drive/MyDrive/KHUDA_winter/out0121_2/frames/frame%04d.png: No such file or directory\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python generate.py --process=interpolation --interpolation=linear --easing=easeInOutQuad --space=w --network=/content/drive/MyDrive/KHUDA_winter/ffhq256-model/out.pkl --outdir=/content/drive/MyDrive/KHUDA_winter/out0121_2/ --projected-w=/content/npz/combined.npz --frames=120"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7RCnSAsWrq"
      },
      "source": [
        "## Feature Extraction using Closed Form Factorization\n",
        "\n",
        "Feature Extraction is the process of finding “human readable” vectors in a StyleGAN model. For example, let’s say you wanted to find a vector that could open or close a mouth in a face model.\n",
        "\n",
        "The feature extractor tries to automate the procss of finding important vectors in your model.\n",
        "\n",
        "`--ckpt`: This is the path to your .pkl file. In other places its called `--network` (It’s a long story for why its name changed here)\n",
        "`--out`: path to save your output feature vector file. The file name must end in `.pt`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hek6TFZCKD-"
      },
      "outputs": [],
      "source": [
        "!python closed_form_factorization.py --out=/content/ladiesblack-cff.pt --ckpt=/content/ladiesblack.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxLgeNeJRqFh"
      },
      "source": [
        "Once this cell is finished you’ll want to save that `.pt` file somewhere for reuse.\n",
        "\n",
        "This process just created the vctor values, but we need to test it on some seed values to determine what each vector actually changes. The `apply_factor.py` script does this.\n",
        "\n",
        "Arguments to try:\n",
        "\n",
        "\n",
        "*   `-i`: This stands for index. By default, the cell above will produce 512 vectors, so `-i` can be any value from 0 to 511. I recommend starting with a higher value.\n",
        "*   `-d`: This stands for degrees. This means how much change you want to see along th vector. I recommend a value between 5 and 10 to start with.\n",
        "*   `--seeds`: You know what these are by now right? :)\n",
        "*   `--ckpt`: path to your .pkl file\n",
        "*   `--video`: adding this to your argument will produce a video that animates your seeds along the vector path. I find it much easier to figure out what’s changing with an animation.\n",
        "*   `--output`: where to save the images/video\n",
        "*   `--space`: By default this will use the w space to reduce entanglement\n",
        "\n",
        "Lastly you need to add the path to the `.pt` file you made in th above cell. It’s weird, but you don’t need to add any arguments bfore it, just make sure its after `apply_factor.pt`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEDSl2VpCSJL"
      },
      "outputs": [],
      "source": [
        "!python apply_factor.py -i 0 -d 10 --seeds 5,10 --ckpt /content/ladiesblack.pkl /content/ladiesblack-cff.pt --output /content/cff-vid/ --video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzwhrjGlTMZ3"
      },
      "source": [
        "That just produced images or video for a single vector, but there are 511 more! To generate every vector, you can uuse the cell below. Update any arguments you want, but don’t touch the `-i {i}` part.\n",
        "\n",
        "**Warning:** This takes a long time, especially if you have more than one seed value (pro tip: don’t usee more than one seed value)! Also, this will take up a good amount of space in Google Drive. You’ve been warned!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aFj6mcKDmqk"
      },
      "outputs": [],
      "source": [
        "for i in range(512):\n",
        "  !python apply_factor.py -i {i} -d 10 --seeds 177 --ckpt /content/drive/MyDrive/network-snapshot-008720.pkl /content/ladies-black-cff.pt --output /content/drive/MyDrive/ladiesblack-cff-17/ --video #--out_prefix 'ladiesblack-factor-{i}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVfmNV5JEcdp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VLRzmilrCf4"
      },
      "source": [
        "# Layer Manipulations\n",
        "\n",
        "The following scripts allow you to modify various resolution layers of the StyleGAN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDpQrBdevrDj"
      },
      "source": [
        "## Flesh Digressions\n",
        "\n",
        "Flesh Digressions works by manipulating the vectors in the base 4x4 layer. By doing this while leaving all the other layers untouched you can create a warping and twisting version of images from your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdvBNkMZv4MK"
      },
      "outputs": [],
      "source": [
        "!python flesh_digression.py --pkl /content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --psi 0.5 --seed 9999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2TrnyvprL42"
      },
      "source": [
        "## Network Blending\n",
        "You can take two completely different models and combine them by splitting them at a specific resolution and combining the lower layers of one model and the higher layers of another.\n",
        "\n",
        "(Note: this tends to work best when one of the models is transfer learned from the other)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6pjl31Jwa4u"
      },
      "outputs": [],
      "source": [
        "!python blend_models.py --lower_res_pkl /content/ffhq-pt.pkl --split_res 64 --higher_res_pkl /content/bone-bone-pt.pkl --output_path /content/ffhq-bonebone-split64.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "futaO6lBroVH"
      },
      "source": [
        "You can now take the output .pkl file and use that with any of the generation tools above."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}